{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce471cb-b9b3-4cb6-83db-b2701c45476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import geopy.distance\n",
    "import time \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e040a6-e15f-402e-9a06-bccdbfd76126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/13 17:56:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/13 17:56:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# initialize sparkSession, make the data from csv to parquet,\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"4 Executors\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0a386b-2819-44ea-87b7-a30800b36769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data into memory, do the necessary joins etc. here\n",
    "crime_data = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/crime_data_*.parquet\")\n",
    "revge = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/revgecoding.parquet\")\n",
    "# only 2015 income data needed\n",
    "income = spark.read \\\n",
    "            .parquet(\"hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet\")\n",
    "lapd_stations = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/LAPD_Police_Stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0ff584-2a36-4021-8284-61a64e7ee10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data = crime_data.withColumn(\"Date Rptd\", to_timestamp(\"Date Rptd\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"Vict Age\", col(\"Vict Age\").cast(\"int\")) \\\n",
    "    .withColumn(\"LAT\", col(\"LAT\").cast(\"double\")) \\\n",
    "    .withColumn(\"LON\", col(\"LON\").cast(\"double\")) \\\n",
    "    .withColumn(\"Premis_Desc\", col(\"Premis Desc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1947c70a-56ae-4263-8317-5997d1e2710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance on a sphere (as earth is not flat)\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371\n",
    "    dLat = math.radians(lat2 - lat1)\n",
    "    dLon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dLat / 2) * math.sin(dLat / 2) + \\\n",
    "        math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dLon / 2) * math.sin(dLon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c433dbf-c878-4bf8-9544-a066c8f7ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 4th query here\n",
    "# 1a)\n",
    "def query_4_1a(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    lapd_stations_new = lapd_stations.withColumnRenamed('PREC','AREA')\n",
    "\n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(broadcast(lapd_stations_new), 'AREA', 'inner')\n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .hint(method).join(lapd_stations_new, 'AREA', 'inner')\n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(lapd_stations_new, 'AREA', 'inner')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('distance',\n",
    "                                    haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(col('Weapon Used Cd') < 200) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('year') \\\n",
    "        .agg((F.sum('distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')) \\\n",
    "       .orderBy(F.col('year'))\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1feb6ccc-b831-403f-935d-e03aa14a55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b)\n",
    "def query_4_1b(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    lapd_stations_new = lapd_stations.withColumnRenamed('PREC','AREA')\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(broadcast(lapd_stations_new), 'AREA', 'inner')\n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .hint(method).join(lapd_stations_new, 'AREA', 'inner')\n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(lapd_stations_new, 'AREA', 'inner')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # distance of LAT and LON using Spark functions\n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('distance',\n",
    "                                    haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\"))) # Earths's radius\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(F.col('Weapon Used Cd').isNotNull()) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('AREA NAME') \\\n",
    "        .agg(\n",
    "            (F.sum('distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')\n",
    "        ) \\\n",
    "        .orderBy(F.col('#').desc()) \\\n",
    "        .withColumnRenamed('AREA NAME', 'division')\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c2f6c-05c6-43af-98bb-676b7000c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+----+\n",
      "|year|  average_distance|   #|\n",
      "+----+------------------+----+\n",
      "|2010| 4.325593300110116|8162|\n",
      "|2011|2.7909872168227494|7225|\n",
      "|2012| 37.45827620685538|6539|\n",
      "|2013| 2.830553808457538|5851|\n",
      "|2014|11.043993584711966|4559|\n",
      "|2015|2.7065460199668823|6729|\n",
      "|2016| 2.718165310899849|8094|\n",
      "|2017| 4.338253959754182|7781|\n",
      "|2018|   2.7360981635515|7414|\n",
      "|2019| 2.741344160752837|7135|\n",
      "|2020|2.3272432584812806|  46|\n",
      "|2021| 37.38241965186492|2553|\n",
      "|2022|2.9681689538445597|  44|\n",
      "|2023| 3.687948331136995|   8|\n",
      "+----+------------------+----+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [year#401 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(year#401 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=351]\n",
      "      +- HashAggregate(keys=[year#401], functions=[sum(distance#329), count(1)])\n",
      "         +- Exchange hashpartitioning(year#401, 200), ENSURE_REQUIREMENTS, [plan_id=348]\n",
      "            +- HashAggregate(keys=[year#401], functions=[partial_sum(distance#329), partial_count(1)])\n",
      "               +- Project [pythonUDF0#503 AS distance#329, year(cast(Date Rptd#80 as date)) AS year#401]\n",
      "                  +- BatchEvalPython [haversine(LON#27, LAT#26, X#68, Y#69)#328], [pythonUDF0#503]\n",
      "                     +- Project [Date Rptd#80, LAT#26, LON#27, X#68, Y#69]\n",
      "                        +- SortMergeJoin [AREA#264], [AREA#256], Inner\n",
      "                           :- Sort [AREA#264 ASC NULLS FIRST], false, 0\n",
      "                           :  +- Exchange hashpartitioning(AREA#264, 200), ENSURE_REQUIREMENTS, [plan_id=338]\n",
      "                           :     +- Project [gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, AREA #4 AS AREA#264, LAT#26, LON#27]\n",
      "                           :        +- Filter ((isnotnull(Weapon Used Cd#16) AND (Weapon Used Cd#16 < 200)) AND isnotnull(AREA #4))\n",
      "                           :           +- FileScan parquet [Date Rptd#1,AREA #4,Weapon Used Cd#16,LAT#26,LON#27] Batched: true, DataFilters: [isnotnull(Weapon Used Cd#16), (Weapon Used Cd#16 < 200), isnotnull(AREA #4)], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [IsNotNull(`Weapon Used Cd`), LessThan(`Weapon Used Cd`,200), IsNotNull(`AREA `)], ReadSchema: struct<Date Rptd:string,AREA :int,Weapon Used Cd:int,LAT:double,LON:double>\n",
      "                           +- Sort [AREA#256 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(AREA#256, 200), ENSURE_REQUIREMENTS, [plan_id=339]\n",
      "                                 +- Project [X#68, Y#69, PREC#73 AS AREA#256]\n",
      "                                    +- Filter isnotnull(PREC#73)\n",
      "                                       +- FileScan parquet [X#68,Y#69,PREC#73] Batched: true, DataFilters: [isnotnull(PREC#73)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/LAPD_Police_Stations.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(PREC)], ReadSchema: struct<X:double,Y:double,PREC:int>\n",
      "\n",
      "\n",
      "Method : MERGE | Time 10.954529762268066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.954529762268066"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4_1a('MERGE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
