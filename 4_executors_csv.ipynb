{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56c50ab-5769-4e7d-946c-ec870888801c",
   "metadata": {},
   "source": [
    "#### Useful Links \n",
    "- Spark History Server : http://83.212.73.248:18080/\n",
    "- Hadoop YARN (scheduler) : http://83.212.73.248:8088/cluster\n",
    "- HDFS : http://83.212.73.248:9870/dfshealth.html#tab-overview\n",
    "\n",
    "#### Useful Commands : \n",
    "- Connect to okeanos-master (from local) : `$ ssh user@snf-40202.ok-kno.grnetcloud.net `\n",
    "    - Password : 'Rand0m'\n",
    "- Connect to okeanos-worker (from okeanos-master) : `$ ssh okeanos-worker`\n",
    "- Open Jupyter Notebook : `$ jupyter notebook --ip 83.212.73.248 --port 8888`\n",
    "\n",
    "#### Thinks to do :\n",
    "- Make the data Csv to Parquet\n",
    "- Make those columns the type we want\n",
    "- Write the Queries (!)\n",
    "- Benchmark and optimize them etc.\n",
    "- Balance the data onto HDFS across the two datanodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667a612-a781-4680-bb05-0fdfe7aed047",
   "metadata": {},
   "source": [
    "### Full HDFS path is here : hdfs://okeanos-master:54310/csv_data/\n",
    "and contains :  \n",
    "     \n",
    "     1.  hdfs://okeanos-master:54310/csv_data/LAPD_Police_Stations.csv\n",
    "     2.  hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv \n",
    "     3.  hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv\n",
    "     4.  hdfs://okeanos-master:54310/csv_data/revgecoding.csv \n",
    "     5.  hdfs://okeanos-master:54310/csv_data/income/\n",
    "         1. LA_income_2015.csv\n",
    "         2. LA_income_2017.csv\n",
    "         3. LA_income_2019.csv\n",
    "         4. LA_income_2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a4e254-eda8-4782-a680-a9fc5c1d69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from operator import add\n",
    "import geopy.distance\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, rank\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbb858d-c65c-40f6-93c8-eae9504bbf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/05 01:56:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/05 01:56:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/01/05 01:56:04 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize sparkSession, make the data from csv to parquet,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4 Executors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.instances\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/spark/python/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/opt/spark/python/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/opt/spark/python/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/opt/spark/python/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/opt/spark/python/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1586\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1578\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1579\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize sparkSession, make the data from csv to parquet,\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"4 Executors\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaaacbe-53a0-4f5d-bf6f-f6eb6c42e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into memory, do the necessary joins etc. here\n",
    "df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header=\"true\") \\\n",
    "    .csv(\"hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv\")\n",
    "df2 = spark.read.options(inferSchema=\"true\", delimiter=\",\", header=\"true\") \\\n",
    "    .csv(\"hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv\")\n",
    "crime_data = df.union(df2)\n",
    "revge = spark.read.options(inferSchema=\"true\", delimiter=\",\", header=\"true\") \\\n",
    "    .csv(\"hdfs://okeanos-master:54310/csv_data/revgecoding.csv\")\n",
    "# only 2015 income data needed\n",
    "income = spark.read \\\n",
    "            .parquet(\"hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet\")\n",
    "lapd_stations = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/LAPD_Police_Stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586a4fc-878b-4370-91e1-36faca721379",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0098ff2-3857-4604-8d16-5c26d003759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ad4d5-4e37-41a5-8833-7b3969368d31",
   "metadata": {},
   "source": [
    "#### Change Column Types \n",
    "Στην εκφωνηση λέει : Διατηρώντας τα αρχικά ονόματα στηλών \n",
    "\n",
    "εννοωντας οτι δεν μπορουμε να κανουμε το 'Date Rptd' -> 'Date_Rptd' ?\n",
    "- Date Rptd: date\n",
    "- DATE OCC: date\n",
    "- Vict Age: integer\n",
    "- LAT: double\n",
    "- LON: double\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85974b-2551-4dda-859a-e86e3011dcea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code for column type changing\n",
    "crime_data = crime_data.withColumn(\"Date Rptd\", to_timestamp(\"Date Rptd\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"Vict Age\", col(\"Vict Age\").cast(\"int\")) \\\n",
    "    .withColumn(\"LAT\", col(\"LAT\").cast(\"double\")) \\\n",
    "    .withColumn(\"LON\", col(\"LON\").cast(\"double\")) \\\n",
    "    .withColumn(\"Premis_Desc\", col(\"Premis Desc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7729edb-fff8-4a31-b7bd-4dd61a4ea834",
   "metadata": {},
   "source": [
    "# 1st Query :\r\n",
    "        find\r\n",
    "            for each year\r\n",
    "                the 3 months with the biggest crime count\r\n",
    "\r\n",
    "        year | month | crime_total (count)  + #order\r\n",
    "        dataframe.show()\r\n",
    "\r\n",
    "        SELECT  YEAR(date_rptd) as year,\r\n",
    "                MONTH(date_rptd) as month,\r\n",
    "                COUNT(*) as crime_total,\r\n",
    "                ROW_NUMBER() OVER (PARTITION BY year ORDER BY crime_total) as '#'\r\n",
    "        GROUP BY YEAR(date_rptd), MONTH(date_rptd)\r\n",
    "        SORT BY year ASC, crime_total DESp    GROUP BY police_station_name\r\n",
    "                ORDER BY #\r\n",
    "\r\n",
    "    2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03eee5-f317-4051-b526-4092b7522187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for first query in SQL API\n",
    "def query_1_SQL_API():\n",
    "    start_time = time.time()\n",
    "    crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT \n",
    "                year(`Date Rptd`) AS year,\n",
    "                month(`Date Rptd`) AS month,\n",
    "                COUNT(*) AS crime_total,\n",
    "                ROW_NUMBER() OVER (PARTITION BY year(`Date Rptd`) ORDER BY COUNT(*) DESC) AS rank\n",
    "            FROM \n",
    "                crime_data\n",
    "            GROUP BY \n",
    "                year(`Date Rptd`), month(`Date Rptd`)\n",
    "        ) ranked_data\n",
    "        WHERE rank <= 3\n",
    "        ORDER BY year, rank\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    result_df = spark.sql(query)\n",
    "    result_df.show()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39898c69-f992-45c8-a534-8ee164828c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for first query in Dataframe API\n",
    "def query_1_Dataframe_API():\n",
    "    start_time = time.time()\n",
    "    crime_counts = crime_data.withColumn(\"year\", F.year(\"Date Rptd\")) \\\n",
    "                          .withColumn(\"month\", F.month(\"Date Rptd\")) \\\n",
    "                          .groupBy(\"year\", \"month\") \\\n",
    "                          .agg(F.count(\"*\").alias(\"crime_total\"))\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"year\").orderBy(F.desc(\"crime_total\"))\n",
    "    \n",
    "    ranked_crime = crime_counts.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    \n",
    "    result_df = ranked_crime.filter(\"rank <= 3\").orderBy(\"year\", \"rank\")\n",
    "    \n",
    "    result_df.show()\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55eda9-3400-4cf9-a5a8-2247792fd456",
   "metadata": {},
   "source": [
    " # 2nd Query :\n",
    "\n",
    "\n",
    "            SELECT street,\n",
    "                   CASE\n",
    "                      WHEN HOUR('Date Rptd') BETWEEN 5 AND 11 THEN 'Morning'\n",
    "                      WHEN HOUR('Date Rptd') BETWEEN 12 AND 16 THEN 'Noon'\n",
    "                      WHEN HOUR('Date Rptd') BETWEEN 17 AND 20 THEN 'Afternoon'\n",
    "                      ELSE 'Night'\n",
    "                    END AS time_group,\n",
    "                    COUNT(*) as count\n",
    "            WHERE 'Prem Desc'='STREET'\n",
    "            GROUP BY time_group\n",
    "            ORDER BY count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6c1df-26e6-4b93-b9ee-a2d5cff7a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 2nd query here for Dataframe/SQL API\n",
    "def query_2_Dataframe_API():\n",
    "    start_time = time.time()\n",
    "    filtered_df = crime_data.filter(crime_data['Premis_Desc'] == 'STREET')\n",
    "\n",
    "    time_group_df = filtered_df.withColumn(\"time_group\",\n",
    "                                       # TIME OCC is in 24 hour military time integer values\n",
    "                                      F.when((F.col('TIME OCC').between(500, 1159)), 'Morning')\n",
    "                                      .when((F.col('TIME OCC').between(1200, 1659)), 'Noon')\n",
    "                                      .when((F.col('TIME OCC').between(1700, 2059)), 'Afternoon')\n",
    "                                      .otherwise('Night'))\n",
    "\n",
    "    result_df = time_group_df.groupBy(\"time_group\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "    result_df = result_df.orderBy(col(\"count\").desc())\n",
    "    result_df.show()\n",
    "    end_time = time.time()\n",
    "    # call explain() method in order\n",
    "    # to see the query's physical plan\n",
    "    # and improve the RDD query\n",
    "    result_df.explain()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e48bb-fb2f-4a6a-acca-c11c76786357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 2nd query here for RDD API\n",
    "def time_segs(row):\n",
    "    if 500 <= int(row['TIME OCC']) <= 1159:\n",
    "        return 'Morning'\n",
    "    elif 1200 <= int(row['TIME OCC']) <= 1659:\n",
    "        return 'Noon'\n",
    "    elif 1700 <= int(row['TIME OCC']) <= 2059:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928f8cf-9407-4a3c-b0cc-836e78e49b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_2_rdd(): \n",
    "    #then broadcast\n",
    "    #spark.sparkContext.broadcast(crime_data)\n",
    "    # changes performance? (37s)\n",
    "\n",
    "    start_time = time.time()\n",
    "    crime_data_rdd = crime_data.rdd.filter(lambda x: x['Premis_Desc'] == 'STREET') \\\n",
    "                                .map(lambda x: (time_segs(x),1)) \\\n",
    "                                .reduceByKey(lambda k1,k2: k1+k2) \\\n",
    "                                .sortBy(lambda x: x[1], ascending = False)\n",
    "    \n",
    "    result = crime_data_rdd.collect()\n",
    "    for time_of_day, count in result:\n",
    "        print(f\"{time_of_day}: {count}\")\n",
    "        \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c51ced-8b88-4c16-ad21-0b1479e9b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time_group(row):\n",
    "    time_occ = int(row)\n",
    "    if 500 <= time_occ <= 1159:\n",
    "        return 'Morning'\n",
    "    elif 1200 <= time_occ <= 1659:\n",
    "        return 'Noon'\n",
    "    elif 1700 <= time_occ <= 2059:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Night'\n",
    "        \n",
    "def query_2_rdd_new():\n",
    "    start_time = time.time()\n",
    "\n",
    "    crime_data_rdd = crime_data.rdd.filter(lambda x: x['Premis_Desc'] == 'STREET') \\\n",
    "                                .map(lambda x: (map_time_group(x['TIME OCC']), 1)) \\\n",
    "                                .reduceByKey(add).sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "    result = crime_data_rdd.collect()\n",
    "    for time_of_day, count in result:\n",
    "        print(f\"{time_of_day}: {count}\")\n",
    "        \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456977f-3b8d-4620-ac19-771aafc23206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_2_Dataframe_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b45bf-c7a1-4f7d-8a85-e8b7827380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_2_rdd_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ad40f-c002-49d6-bc68-230131410474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c962e-49f3-4409-aa0e-3b113c973570",
   "metadata": {},
   "source": [
    "# 3rd Query :\n",
    "\n",
    "        find the 3 zip codes with min and max household income\n",
    "                    |\n",
    "                    |\n",
    "                    v\n",
    "        // filter(remove) victimless crimes\n",
    "                    |\n",
    "                    |\n",
    "                    v\n",
    "        select vict_desc, COUNT(*) as count\n",
    "        where year=2015\n",
    "        group by vict_desc\n",
    "        order by count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006e0c2-4c2d-4541-ac68-10bad330ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 3rd query here\n",
    "def query_3(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_revge = crime_data.join(broadcast(revge), ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        crime_data_join_revge.explain()\n",
    "        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_revge = crime_data.hint(method).join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        crime_data_join_revge.explain()\n",
    "        \n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_revge = crime_data.join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    #crime_data_join_revge = crime_data.join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "    #    .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "    #    .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "    #    .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "    \n",
    "    crime_data_join_income = crime_data_join_revge.join(income, 'Zip Code', 'inner') \\\n",
    "                                .withColumn('Estimated Median Income', \n",
    "                                            regexp_replace(col('Estimated Median Income'), '[$,]', '')) \\\n",
    "                                .withColumn('Estimated Median Income', \n",
    "                                            col('Estimated Median Income') \\\n",
    "                                .cast('double'))\n",
    "    \n",
    "    max_income_zip_codes = crime_data_join_income.groupBy('Zip Code') \\\n",
    "                            .agg({'Estimated Median Income': 'max'}) \\\n",
    "                            .withColumnRenamed('max(Estimated Median Income)', 'MaxIncome') \\\n",
    "                            .orderBy(col('MaxIncome').desc()) \\\n",
    "                            .limit(3)\n",
    "    \n",
    "    min_income_zip_codes = crime_data_join_income.groupBy('Zip Code') \\\n",
    "                            .agg({'Estimated Median Income': 'min'}) \\\n",
    "                            .withColumnRenamed('min(Estimated Median Income)', 'MinIncome') \\\n",
    "                            .orderBy(col('MinIncome')) \\\n",
    "                            .limit(3)\n",
    "    \n",
    "    zip_codes = min_income_zip_codes.union(max_income_zip_codes)\n",
    "    \n",
    "    zip_codes_list = [row['Zip Code'] for row in zip_codes.collect()]\n",
    "    \n",
    "    result = crime_data_join_income \\\n",
    "                .filter(col('Zip Code').isin(zip_codes_list)) \\\n",
    "                .filter(year(col('Date Rptd')) == 2015) \\\n",
    "                .groupBy('Vict Descent') \\\n",
    "                .count() \\\n",
    "                .withColumnRenamed('count', '#') \\\n",
    "                .orderBy(col('#').desc())\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48047e1-8739-48ef-86b3-1cc547d2beb1",
   "metadata": {},
   "source": [
    "# 4th Query :\n",
    "    1 :\n",
    "        a)\n",
    "                make_extra_columns() : distance of police stations from crime\n",
    "                join crime_table with LA Police Stations on police_station\n",
    "                and add column of police station\n",
    "                for each row compute distance from two coordinates                         \n",
    "                put this computed distance in the column named 'distance'\n",
    "\n",
    "                SELECT year, SUM(distance)/# as average_distance\n",
    "                                , COUNT(*) as #\n",
    "                FROM ...\n",
    "                WHERE WEAPON < 200\n",
    "                GROUP BY year\n",
    "                ORDER BY #\n",
    "\n",
    "\n",
    "        b)\n",
    "                SELECT police_station_name as division,\n",
    "                       SUM(distance)/# as average_distance,\n",
    "                       COUNT(*) as #\n",
    "                FROM ...\n",
    "                WHERE weapon NOT NULL\n",
    "                GROUP BY police_station_name\n",
    "                ORDER BY #\n",
    "\n",
    "    2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8c250-8820-4bb9-84de-b83bd512fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371\n",
    "    dLat = math.radians(lat2 - lat1)\n",
    "    dLon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dLat / 2) * math.sin(dLat / 2) + \\\n",
    "        math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dLon / 2) * math.sin(dLon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a08e7-d25b-4e9c-af0c-c33b7c15f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 4th query here\n",
    "# 1a)\n",
    "def query_4_1a(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    lapd_stations_new = lapd_stations.withColumnRenamed('PREC','AREA')\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(broadcast(lapd_stations_new), 'AREA', 'inner')\n",
    "        crime_data_join_stations.explain()\n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .hint(method).join(lapd_stations_new, 'AREA', 'inner')\n",
    "        crime_data_join_stations.explain()\n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(lapd_stations_new, 'AREA', 'inner')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('distance',\n",
    "                                    haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(col('Weapon Used Cd') < 200) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('year') \\\n",
    "        .agg((F.sum('distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')) \\\n",
    "       .orderBy(F.col('year'))\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1887d39-8279-4ee0-befe-2e1117b5d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b)\n",
    "def query_4_1b(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    lapd_stations_new = lapd_stations.withColumnRenamed('PREC','AREA')\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(broadcast(lapd_stations_new), 'AREA', 'inner')\n",
    "        crime_data_join_stations.explain()\n",
    "        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .hint(method).join(lapd_stations_new, 'AREA', 'inner')\n",
    "        crime_data_join_stations.explain()\n",
    "        \n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(lapd_stations_new, 'AREA', 'inner')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # distance of LAT and LON using Spark functions\n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('distance',\n",
    "                                    haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\"))) # Earths's radius\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(F.col('Weapon Used Cd').isNotNull()) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('AREA NAME') \\\n",
    "        .agg(\n",
    "            (F.sum('distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')\n",
    "        ) \\\n",
    "        .orderBy(F.col('#').desc()) \\\n",
    "        .withColumnRenamed('AREA NAME', 'division')\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b1f43-dd42-4ab4-89fc-100b2de20849",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987774a5-2c90-4679-9a1d-3b3d7f93864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a)\n",
    "def query_4_2a(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        combined_data = crime_data.hint(method).crossJoin(broadcast(lapd_stations))\n",
    "        combined_data.explain()\n",
    "        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        combined_data = crime_data.hint(method).crossJoin(lapd_stations)\n",
    "        combined_data.explain()\n",
    "        \n",
    "    elif method == 'CONTINUE':\n",
    "        combined_data = crime_data.crossJoin(lapd_stations)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # filter for DR_NO < 200  \n",
    "    # if lazy evaluation/optimizer doesnt do it by itself\n",
    "    \n",
    "    combined_data = combined_data.withColumn(\"closest_distance\", haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    windowSpec = Window.partitionBy(\"DR_NO\").orderBy(\"closest_distance\")\n",
    "    closest_stations = combined_data.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") == 1)\n",
    "    \n",
    "    final_data = closest_stations.select(col(\"DR_NO\"), col(\"DIVISION\").alias(\"closest_station\"), col(\"closest_distance\"))\n",
    "    \n",
    "    result = crime_data.join(final_data, \"DR_NO\")\n",
    "    crime_data_join_stations = result.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(col('Weapon Used Cd') < 200) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('year') \\\n",
    "        .agg((F.sum('closest_distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')) \\\n",
    "       .orderBy(F.col('year'))\n",
    "\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27dfc9d-94e5-4d8c-874a-7a6872692fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a)\n",
    "def query_4_2b(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        combined_data = crime_data.hint(method).crossJoin(broadcast(lapd_stations))\n",
    "        combined_data.explain()\n",
    "        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        combined_data = crime_data.hint(method).crossJoin(lapd_stations)\n",
    "        combined_data.explain()\n",
    "        \n",
    "    elif method == 'CONTINUE':\n",
    "        combined_data = crime_data.crossJoin(lapd_stations)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    combined_data = combined_data.withColumn(\"closest_distance\", haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    windowSpec = Window.partitionBy(\"DR_NO\").orderBy(\"closest_distance\")\n",
    "    closest_stations = combined_data.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") == 1)\n",
    "    \n",
    "    final_data = closest_stations.select(col(\"DR_NO\"), col(\"DIVISION\").alias(\"closest_station\"), col(\"closest_distance\"))\n",
    "    \n",
    "    result = crime_data.join(final_data, \"DR_NO\")\n",
    "    crime_data_join_stations = result.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(F.col('Weapon Used Cd').isNotNull()) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('AREA NAME') \\\n",
    "        .agg(\n",
    "            (F.sum('closest_distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')\n",
    "        ) \\\n",
    "        .orderBy(F.col('#').desc()) \\\n",
    "        .withColumnRenamed('AREA NAME', 'division')\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc31421-b725-4649-8b65-5f6308026e91",
   "metadata": {},
   "source": [
    "# Query 1 on 4 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175a3f8-6285-4e40-8abe-f4d64c010ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_1_Dataframe_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d0ad43-b4f9-4843-ae89-c2f49b003d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:================================>                         (5 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----+\n",
      "|year|month|crime_total|rank|\n",
      "+----+-----+-----------+----+\n",
      "|2010|    3|      17595|   1|\n",
      "|2010|    7|      17520|   2|\n",
      "|2010|    5|      17338|   3|\n",
      "|2011|    8|      17139|   1|\n",
      "|2011|    5|      17050|   2|\n",
      "|2011|    3|      16951|   3|\n",
      "|2012|    8|      17696|   1|\n",
      "|2012|   10|      17477|   2|\n",
      "|2012|    5|      17391|   3|\n",
      "|2013|    8|      17329|   1|\n",
      "|2013|    7|      16714|   2|\n",
      "|2013|    5|      16671|   3|\n",
      "|2014|    7|      14059|   1|\n",
      "|2014|   10|      14031|   2|\n",
      "|2014|    9|      13799|   3|\n",
      "|2015|    8|      18951|   1|\n",
      "|2015|   10|      18916|   2|\n",
      "|2015|    7|      18528|   3|\n",
      "|2016|    8|      19779|   1|\n",
      "|2016|   10|      19615|   2|\n",
      "+----+-----+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3756721019744873"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1_SQL_API()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e5649-89cf-4d22-bcca-f0c76f039ddf",
   "metadata": {},
   "source": [
    "# Query 2 on 4 Executors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98250c68-967e-46d6-a097-cf0e07c9b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:============================================>            (7 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|time_group| count|\n",
      "+----------+------+\n",
      "|     Night|237605|\n",
      "| Afternoon|187306|\n",
      "|      Noon|148180|\n",
      "|   Morning|123846|\n",
      "+----------+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#10552L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#10552L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=3709]\n",
      "      +- HashAggregate(keys=[time_group#10490], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(time_group#10490, 200), ENSURE_REQUIREMENTS, [plan_id=3706]\n",
      "            +- HashAggregate(keys=[time_group#10490], functions=[partial_count(1)])\n",
      "               +- Project [CASE WHEN ((TIME OCC#20 >= 500) AND (TIME OCC#20 <= 1159)) THEN Morning WHEN ((TIME OCC#20 >= 1200) AND (TIME OCC#20 <= 1659)) THEN Noon WHEN ((TIME OCC#20 >= 1700) AND (TIME OCC#20 <= 2059)) THEN Afternoon ELSE Night END AS time_group#10490]\n",
      "                  +- Filter (isnotnull(Premis_Desc#557) AND (Premis_Desc#557 = STREET))\n",
      "                     +- InMemoryTableScan [Premis_Desc#557, TIME OCC#20], [isnotnull(Premis_Desc#557), (Premis_Desc#557 = STREET)]\n",
      "                           +- InMemoryRelation [DR_NO#17, Date Rptd#412, DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#470, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, Crm Cd 3#39, Crm Cd 4#40, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- Union\n",
      "                                    :- *(1) Project [DR_NO#17, gettimestamp(Date Rptd#18, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#412, gettimestamp(DATE OCC#19, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, Crm Cd 3#39, Crm Cd 4#40, ... 5 more fields]\n",
      "                                    :  +- FileScan csv [DR_NO#17,Date Rptd#18,DATE OCC#19,TIME OCC#20,AREA #21,AREA NAME#22,Rpt Dist No#23,Part 1-2#24,Crm Cd#25,Crm Cd Desc#26,Mocodes#27,Vict Age#28,Vict Sex#29,Vict Descent#30,Premis Cd#31,Premis Desc#32,Weapon Used Cd#33,Weapon Desc#34,Status#35,Status Desc#36,Crm Cd 1#37,Crm Cd 2#38,Crm Cd 3#39,Crm Cd 4#40,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "                                    +- *(2) Project [DR_NO#90, gettimestamp(Date Rptd#91, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#6131, gettimestamp(DATE OCC#92, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#6132, TIME OCC#93, AREA#94, AREA NAME#95, Rpt Dist No#96, Part 1-2#97, Crm Cd#98, Crm Cd Desc#99, Mocodes#100, Vict Age#101, Vict Sex#102, Vict Descent#103, Premis Cd#104, Premis Desc#105, Weapon Used Cd#106, Weapon Desc#107, Status#108, Status Desc#109, Crm Cd 1#110, Crm Cd 2#111, Crm Cd 3#112, Crm Cd 4#113, ... 5 more fields]\n",
      "                                       +- FileScan csv [DR_NO#90,Date Rptd#91,DATE OCC#92,TIME OCC#93,AREA#94,AREA NAME#95,Rpt Dist No#96,Part 1-2#97,Crm Cd#98,Crm Cd Desc#99,Mocodes#100,Vict Age#101,Vict Sex#102,Vict Descent#103,Premis Cd#104,Premis Desc#105,Weapon Used Cd#106,Weapon Desc#107,Status#108,Status Desc#109,Crm Cd 1#110,Crm Cd 2#111,Crm Cd 3#112,Crm Cd 4#113,... 4 more fields] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.562849998474121"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2_Dataframe_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05b4d4b6-a5ca-4d86-93c0-72355d45a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night: 237605\n",
      "Afternoon: 187306\n",
      "Noon: 148180\n",
      "Morning: 123846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.062253952026367"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2_rdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a199f4e-af1a-4044-aba1-9de3e6dcd2f1",
   "metadata": {},
   "source": [
    "# Query 3 on 4 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c4e344e-6eab-480b-b6f8-920f5a78291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1556|\n",
      "|           B|1092|\n",
      "|           W|1002|\n",
      "|           O| 484|\n",
      "|           A| 116|\n",
      "|           K|   7|\n",
      "|           I|   3|\n",
      "|           J|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "Method : CONTINUE | Time 20.74992871284485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.74992871284485"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b6323b9-6b57-475f-8595-aaaeb1223108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [LAT#43, LON#44, DR_NO#17, Date Rptd#412, DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, ... 6 more fields]\n",
      "   +- BroadcastHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44))], [knownfloatingpointnormalized(normalizenanandzero(LAT#191)), knownfloatingpointnormalized(normalizenanandzero(LON#192))], Inner, BuildRight, false\n",
      "      :- Union\n",
      "      :  :- Project [DR_NO#17, gettimestamp(Date Rptd#18, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#412, gettimestamp(DATE OCC#19, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, Crm Cd 3#39, Crm Cd 4#40, ... 5 more fields]\n",
      "      :  :  +- Filter (((((isnotnull(Vict Descent#30) AND isnotnull(Vict Sex#29)) AND NOT (Vict Descent#30 = X)) AND NOT (Vict Sex#29 = X)) AND isnotnull(LAT#43)) AND isnotnull(LON#44))\n",
      "      :  :     +- FileScan csv [DR_NO#17,Date Rptd#18,DATE OCC#19,TIME OCC#20,AREA #21,AREA NAME#22,Rpt Dist No#23,Part 1-2#24,Crm Cd#25,Crm Cd Desc#26,Mocodes#27,Vict Age#28,Vict Sex#29,Vict Descent#30,Premis Cd#31,Premis Desc#32,Weapon Used Cd#33,Weapon Desc#34,Status#35,Status Desc#36,Crm Cd 1#37,Crm Cd 2#38,Crm Cd 3#39,Crm Cd 4#40,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#30), isnotnull(Vict Sex#29), NOT (Vict Descent#30 = X), NOT (Vict Sex#29 ..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(Vict Sex), Not(EqualTo(Vict Descent,X)), Not(EqualTo(Vict Sex..., ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "      :  +- Project [DR_NO#90, gettimestamp(Date Rptd#91, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#1423, gettimestamp(DATE OCC#92, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#1424, TIME OCC#93, AREA#94, AREA NAME#95, Rpt Dist No#96, Part 1-2#97, Crm Cd#98, Crm Cd Desc#99, Mocodes#100, Vict Age#101, Vict Sex#102, Vict Descent#103, Premis Cd#104, Premis Desc#105, Weapon Used Cd#106, Weapon Desc#107, Status#108, Status Desc#109, Crm Cd 1#110, Crm Cd 2#111, Crm Cd 3#112, Crm Cd 4#113, ... 5 more fields]\n",
      "      :     +- Filter (((((isnotnull(Vict Descent#103) AND isnotnull(Vict Sex#102)) AND NOT (Vict Descent#103 = X)) AND NOT (Vict Sex#102 = X)) AND isnotnull(LAT#116)) AND isnotnull(LON#117))\n",
      "      :        +- FileScan csv [DR_NO#90,Date Rptd#91,DATE OCC#92,TIME OCC#93,AREA#94,AREA NAME#95,Rpt Dist No#96,Part 1-2#97,Crm Cd#98,Crm Cd Desc#99,Mocodes#100,Vict Age#101,Vict Sex#102,Vict Descent#103,Premis Cd#104,Premis Desc#105,Weapon Used Cd#106,Weapon Desc#107,Status#108,Status Desc#109,Crm Cd 1#110,Crm Cd 2#111,Crm Cd 3#112,Crm Cd 4#113,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#103), isnotnull(Vict Sex#102), NOT (Vict Descent#103 = X), NOT (Vict Sex#..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(Vict Sex), Not(EqualTo(Vict Descent,X)), Not(EqualTo(Vict Sex..., ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(knownfloatingpointnormalized(normalizenanandzero(input[0, double, false])), knownfloatingpointnormalized(normalizenanandzero(input[1, double, false]))),false), [plan_id=1744]\n",
      "         +- Filter (isnotnull(LAT#191) AND isnotnull(LON#192))\n",
      "            +- FileScan csv [LAT#191,LON#192,ZIPcode#193] Batched: false, DataFilters: [isnotnull(LAT#191), isnotnull(LON#192)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/revgecoding.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1556|\n",
      "|           B|1092|\n",
      "|           W|1002|\n",
      "|           O| 484|\n",
      "|           A| 116|\n",
      "|           K|   7|\n",
      "|           I|   3|\n",
      "|           J|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "Method : BROADCAST | Time 17.158113718032837\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [LAT#43, LON#44, DR_NO#17, Date Rptd#412, DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, ... 6 more fields]\n",
      "   +- SortMergeJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44))], [knownfloatingpointnormalized(normalizenanandzero(LAT#191)), knownfloatingpointnormalized(normalizenanandzero(LON#192))], Inner\n",
      "      :- Sort [knownfloatingpointnormalized(normalizenanandzero(LAT#43)) ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(LON#44)) ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44)), 200), ENSURE_REQUIREMENTS, [plan_id=2811]\n",
      "      :     +- Union\n",
      "      :        :- Project [DR_NO#17, gettimestamp(Date Rptd#18, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#412, gettimestamp(DATE OCC#19, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, Crm Cd 3#39, Crm Cd 4#40, ... 5 more fields]\n",
      "      :        :  +- Filter (((((isnotnull(Vict Descent#30) AND isnotnull(Vict Sex#29)) AND NOT (Vict Descent#30 = X)) AND NOT (Vict Sex#29 = X)) AND isnotnull(LAT#43)) AND isnotnull(LON#44))\n",
      "      :        :     +- FileScan csv [DR_NO#17,Date Rptd#18,DATE OCC#19,TIME OCC#20,AREA #21,AREA NAME#22,Rpt Dist No#23,Part 1-2#24,Crm Cd#25,Crm Cd Desc#26,Mocodes#27,Vict Age#28,Vict Sex#29,Vict Descent#30,Premis Cd#31,Premis Desc#32,Weapon Used Cd#33,Weapon Desc#34,Status#35,Status Desc#36,Crm Cd 1#37,Crm Cd 2#38,Crm Cd 3#39,Crm Cd 4#40,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#30), isnotnull(Vict Sex#29), NOT (Vict Descent#30 = X), NOT (Vict Sex#29 ..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(Vict Sex), Not(EqualTo(Vict Descent,X)), Not(EqualTo(Vict Sex..., ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "      :        +- Project [DR_NO#90, gettimestamp(Date Rptd#91, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#1857, gettimestamp(DATE OCC#92, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#1858, TIME OCC#93, AREA#94, AREA NAME#95, Rpt Dist No#96, Part 1-2#97, Crm Cd#98, Crm Cd Desc#99, Mocodes#100, Vict Age#101, Vict Sex#102, Vict Descent#103, Premis Cd#104, Premis Desc#105, Weapon Used Cd#106, Weapon Desc#107, Status#108, Status Desc#109, Crm Cd 1#110, Crm Cd 2#111, Crm Cd 3#112, Crm Cd 4#113, ... 5 more fields]\n",
      "      :           +- Filter (((((isnotnull(Vict Descent#103) AND isnotnull(Vict Sex#102)) AND NOT (Vict Descent#103 = X)) AND NOT (Vict Sex#102 = X)) AND isnotnull(LAT#116)) AND isnotnull(LON#117))\n",
      "      :              +- FileScan csv [DR_NO#90,Date Rptd#91,DATE OCC#92,TIME OCC#93,AREA#94,AREA NAME#95,Rpt Dist No#96,Part 1-2#97,Crm Cd#98,Crm Cd Desc#99,Mocodes#100,Vict Age#101,Vict Sex#102,Vict Descent#103,Premis Cd#104,Premis Desc#105,Weapon Used Cd#106,Weapon Desc#107,Status#108,Status Desc#109,Crm Cd 1#110,Crm Cd 2#111,Crm Cd 3#112,Crm Cd 4#113,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#103), isnotnull(Vict Sex#102), NOT (Vict Descent#103 = X), NOT (Vict Sex#..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(Vict Sex), Not(EqualTo(Vict Descent,X)), Not(EqualTo(Vict Sex..., ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "      +- Sort [knownfloatingpointnormalized(normalizenanandzero(LAT#191)) ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(LON#192)) ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#191)), knownfloatingpointnormalized(normalizenanandzero(LON#192)), 200), ENSURE_REQUIREMENTS, [plan_id=2812]\n",
      "            +- Filter (isnotnull(LAT#191) AND isnotnull(LON#192))\n",
      "               +- FileScan csv [LAT#191,LON#192,ZIPcode#193] Batched: false, DataFilters: [isnotnull(LAT#191), isnotnull(LON#192)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/revgecoding.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1556|\n",
      "|           B|1092|\n",
      "|           W|1002|\n",
      "|           O| 484|\n",
      "|           A| 116|\n",
      "|           K|   7|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "Method : MERGE | Time 18.987554788589478\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [LAT#43, LON#44, DR_NO#17, Date Rptd#412, DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, ... 6 more fields]\n",
      "   +- ShuffledHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44))], [knownfloatingpointnormalized(normalizenanandzero(LAT#191)), knownfloatingpointnormalized(normalizenanandzero(LON#192))], Inner, BuildLeft\n",
      "      :- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44)), 200), ENSURE_REQUIREMENTS, [plan_id=4042]\n",
      "      :  +- Union\n",
      "      :     :- Project [DR_NO#17, gettimestamp(Date Rptd#18, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#412, gettimestamp(DATE OCC#19, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#441, TIME OCC#20, AREA #21, AREA NAME#22, Rpt Dist No#23, Part 1-2#24, Crm Cd#25, Crm Cd Desc#26, Mocodes#27, Vict Age#28, Vict Sex#29, Vict Descent#30, Premis Cd#31, Premis Desc#32, Weapon Used Cd#33, Weapon Desc#34, Status#35, Status Desc#36, Crm Cd 1#37, Crm Cd 2#38, Crm Cd 3#39, Crm Cd 4#40, ... 5 more fields]\n",
      "      :     :  +- Filter (((((isnotnull(Vict Descent#30) AND isnotnull(Vict Sex#29)) AND NOT (Vict Descent#30 = X)) AND NOT (Vict Sex#29 = X)) AND isnotnull(LAT#43)) AND isnotnull(LON#44))\n",
      "      :     :     +- FileScan csv [DR_NO#17,Date Rptd#18,DATE OCC#19,TIME OCC#20,AREA #21,AREA NAME#22,Rpt Dist No#23,Part 1-2#24,Crm Cd#25,Crm Cd Desc#26,Mocodes#27,Vict Age#28,Vict Sex#29,Vict Descent#30,Premis Cd#31,Premis Desc#32,Weapon Used Cd#33,Weapon Desc#34,Status#35,Status Desc#36,Crm Cd 1#37,Crm Cd 2#38,Crm Cd 3#39,Crm Cd 4#40,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#30), isnotnull(Vict Sex#29), NOT (Vict Descent#30 = X), NOT (Vict Sex#29 ..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(Vict Sex), Not(EqualTo(Vict Descent,X)), Not(EqualTo(Vict Sex..., ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "      :     +- Project [DR_NO#90, gettimestamp(Date Rptd#91, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#2283, gettimestamp(DATE OCC#92, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#2284, TIME OCC#93, AREA#94, AREA NAME#95, Rpt Dist No#96, Part 1-2#97, Crm Cd#98, Crm Cd Desc#99, Mocodes#100, Vict Age#101, Vict Sex#102, Vict Descent#103, Premis Cd#104, Premis Desc#105, Weapon Used Cd#106, Weapon Desc#107, Status#108, Status Desc#109, Crm Cd 1#110, Crm Cd 2#111, Crm Cd 3#112, Crm Cd 4#113, ... 5 more fields]\n",
      "      :        +- Filter (((((isnotnull(Vict Descent#103) AND isnotnull(Vict Sex#102)) AND NOT (Vict Descent#103 = X)) AND NOT (Vict Sex#102 = X)) AND isnotnull(LAT#116)) AND isnotnull(LON#117))\n",
      "      :           +- FileScan csv [DR_NO#90,Date Rptd#91,DATE OCC#92,TIME OCC#93,AREA#94,AREA NAME#95,Rpt Dist No#96,Part 1-2#97,Crm Cd#98,Crm Cd Desc#99,Mocodes#100,Vict Age#101,Vict Sex#102,Vict Descent#103,Premis Cd#104,Premis Desc#105,Weapon Used Cd#106,Weapon Desc#107,Status#108,Status Desc#109,Crm Cd 1#110,Crm Cd 2#111,Crm Cd 3#112,Crm Cd 4#113,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#103), isnotnull(Vict Sex#102), NOT (Vict Descent#103 = X), NOT (Vict Sex#..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(Vict Sex), Not(EqualTo(Vict Descent,X)), Not(EqualTo(Vict Sex..., ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA:int,AREA NAME:string,Rpt Dist...\n",
      "      +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#191)), knownfloatingpointnormalized(normalizenanandzero(LON#192)), 200), ENSURE_REQUIREMENTS, [plan_id=4043]\n",
      "         +- Filter (isnotnull(LAT#191) AND isnotnull(LON#192))\n",
      "            +- FileScan csv [LAT#191,LON#192,ZIPcode#193] Batched: false, DataFilters: [isnotnull(LAT#191), isnotnull(LON#192)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/csv_data/revgecoding.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1556|\n",
      "|           B|1092|\n",
      "|           W|1002|\n",
      "|           O| 484|\n",
      "|           A| 116|\n",
      "|           K|   7|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "Method : SHUFFLE_HASH | Time 14.167540550231934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_3(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfaa40-3215-4fb1-9c20-d6e1831c26ba",
   "metadata": {},
   "source": [
    "# Query 4 on 4 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b56f3227-7e31-41ce-9fe2-3ad13c94bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----+\n",
      "|year|  average_distance|    #|\n",
      "+----+------------------+-----+\n",
      "|2010|4.3255933001101114| 8162|\n",
      "|2011|2.7909872168227423| 7225|\n",
      "|2012| 37.45827620685533| 6539|\n",
      "|2013| 2.830553808457538| 5851|\n",
      "|2014|11.043993584711998| 4559|\n",
      "|2015| 2.706546019966876| 6729|\n",
      "|2016| 2.718165310899851| 8094|\n",
      "|2017|4.3382539597541765| 7781|\n",
      "|2018|2.7360981635514983| 7414|\n",
      "|2019| 2.741344160752832| 7135|\n",
      "|2020| 8.609530452758886| 8496|\n",
      "|2021|32.313062258505404|12316|\n",
      "|2022|2.6126264414998865|10067|\n",
      "|2023|2.5497025432007963| 8951|\n",
      "+----+------------------+-----+\n",
      "\n",
      "Method : CONTINUE | Time 4.756443738937378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.756443738937378"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4_1a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb4c2627-39c0-4a25-aedc-b45189117425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----+\n",
      "|   division|  average_distance|    #|\n",
      "+-----------+------------------+-----+\n",
      "|77th Street| 13.13902769776695|94679|\n",
      "|  Southeast|18.719601477645817|77917|\n",
      "|  Southwest| 9.881379517897534|72632|\n",
      "|    Central|23.391588077036392|63476|\n",
      "|     Newton|13.952382592339601|61300|\n",
      "|    Rampart|19.798620859447436|55761|\n",
      "|    Olympic| 24.55548058034261|52957|\n",
      "|  Hollywood| 27.77383276714913|51099|\n",
      "|    Mission|26.641751598492025|43604|\n",
      "|    Pacific|25.005205895832077|42897|\n",
      "| Hollenbeck|19.564855074679432|41478|\n",
      "|     Harbor|14.131988077207227|40746|\n",
      "|N Hollywood|17.515613806281824|40345|\n",
      "|   Wilshire|16.038118359629955|37830|\n",
      "|  Northeast|12.772470868243007|37210|\n",
      "|   Foothill|20.930925147358497|36917|\n",
      "|   Van Nuys|19.881787526384876|36172|\n",
      "|    Topanga| 6.782413458606602|34703|\n",
      "|West Valley|15.296850408934183|33829|\n",
      "| Devonshire| 19.09195315013003|32486|\n",
      "+-----------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Method : CONTINUE | Time 5.1063995361328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.1063995361328125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4_1b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fccae6-016c-4a45-8a07-888ea44ae9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4_2a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029fb8d-319d-4e31-a8e6-69b97c06722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4_2b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9b5b2-94ae-4cb5-80d2-cb8f8775152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_1a(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffad6f-3f43-4660-98ff-44ff85fd42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_1b(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15089bd1-65f3-47b0-a2ae-1b1e5030aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_2a(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d14338-4bda-41cb-b43d-5731af568ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_2b(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08674079-e23f-46b1-b5b9-30c9c3f868a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
