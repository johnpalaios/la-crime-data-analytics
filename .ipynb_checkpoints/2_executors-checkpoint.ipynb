{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56c50ab-5769-4e7d-946c-ec870888801c",
   "metadata": {},
   "source": [
    "#### Useful Links \n",
    "- Spark History Server : http://83.212.73.248:18080/\n",
    "- Hadoop YARN (scheduler) : http://83.212.73.248:8088/cluster\n",
    "- HDFS : http://83.212.73.248:9870/dfshealth.html#tab-overview\n",
    "\n",
    "#### Useful Commands : \n",
    "- Connect to okeanos-master (from local) : `$ ssh user@snf-40202.ok-kno.grnetcloud.net `\n",
    "    - Password : 'Rand0m'\n",
    "- Connect to okeanos-worker (from okeanos-master) : `$ ssh okeanos-worker`\n",
    "- Open Jupyter Notebook : `$ jupyter notebook --ip 83.212.73.248 --port 8888`\n",
    "\n",
    "#### Thinks to do :\n",
    "- Make the data Csv to Parquet\n",
    "- Make those columns the type we want\n",
    "- Write the Queries (!)\n",
    "- Benchmark and optimize them etc.\n",
    "- Balance the data onto HDFS across the two datanodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667a612-a781-4680-bb05-0fdfe7aed047",
   "metadata": {},
   "source": [
    "### Full HDFS path is here : hdfs://okeanos-master:54310/csv_data/\n",
    "and contains :  \n",
    "     \n",
    "     1.  hdfs://okeanos-master:54310/csv_data/LAPD_Police_Stations.csv\n",
    "     2.  hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv \n",
    "     3.  hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv\n",
    "     4.  hdfs://okeanos-master:54310/csv_data/revgecoding.csv \n",
    "     5.  hdfs://okeanos-master:54310/csv_data/income/\n",
    "         1. LA_income_2015.csv\n",
    "         2. LA_income_2017.csv\n",
    "         3. LA_income_2019.csv\n",
    "         4. LA_income_2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a4e254-eda8-4782-a680-a9fc5c1d69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import geopy.distance\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbb858d-c65c-40f6-93c8-eae9504bbf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/06 20:08:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/06 20:08:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# initialize sparkSession, make the data from csv to parquet,\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"2 Executors\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efaaacbe-53a0-4f5d-bf6f-f6eb6c42e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data into memory, do the necessary joins etc. here\n",
    "crime_data = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/crime_data_*.parquet\")\n",
    "revge = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/revgecoding.parquet\")\n",
    "# only 2015 income data needed\n",
    "income = spark.read \\\n",
    "            .parquet(\"hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet\")\n",
    "lapd_stations = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/LAPD_Police_Stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53fdc1d-0fa1-4f46-ac36-9605adb844bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executor.instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b586a4fc-878b-4370-91e1-36faca721379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crime_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0098ff2-3857-4604-8d16-5c26d003759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ad4d5-4e37-41a5-8833-7b3969368d31",
   "metadata": {},
   "source": [
    "#### Change Column Types \n",
    "Στην εκφωνηση λέει : Διατηρώντας τα αρχικά ονόματα στηλών \n",
    "\n",
    "εννοωντας οτι δεν μπορουμε να κανουμε το 'Date Rptd' -> 'Date_Rptd' ?\n",
    "- Date Rptd: date\n",
    "- DATE OCC: date\n",
    "- Vict Age: integer\n",
    "- LAT: double\n",
    "- LON: double\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b85974b-2551-4dda-859a-e86e3011dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for column type changing\n",
    "crime_data = crime_data.withColumn(\"Date Rptd\", to_timestamp(\"Date Rptd\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"Vict Age\", col(\"Vict Age\").cast(\"int\")) \\\n",
    "    .withColumn(\"LAT\", col(\"LAT\").cast(\"double\")) \\\n",
    "    .withColumn(\"LON\", col(\"LON\").cast(\"double\")) \\\n",
    "    .withColumn(\"Premis_Desc\", col(\"Premis Desc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c962e-49f3-4409-aa0e-3b113c973570",
   "metadata": {},
   "source": [
    "# 3rd Query :\n",
    "\n",
    "        find the 3 zip codes with min and max household income\n",
    "                    |\n",
    "                    |\n",
    "                    v\n",
    "        // filter(remove) victimless crimes\n",
    "                    |\n",
    "                    |\n",
    "                    v\n",
    "        select vict_desc, COUNT(*) as count\n",
    "        where year=2015\n",
    "        group by vict_desc\n",
    "        order by count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca8975de-3a0a-4735-a99c-92f24516b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 3rd query here\n",
    "def query_3(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    #crime_data_2015 = crime_data.filter(year(col('Date Rptd')) == 2015)\n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_revge = crime_data.join(broadcast(revge), ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        #crime_data_join_revge.explain()\n",
    "        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_revge = crime_data.hint(method).join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        #crime_data_join_revge.explain()\n",
    "        \n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_revge = crime_data.join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    #crime_data_join_revge = crime_data.join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "    #    .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "    #    .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "    #    .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "    \n",
    "    crime_data_join_income = crime_data_join_revge.join(income, 'Zip Code', 'inner') \\\n",
    "                                .withColumn('Estimated Median Income', \n",
    "                                            regexp_replace(col('Estimated Median Income'), '[$,]', '')) \\\n",
    "                                .withColumn('Estimated Median Income', \n",
    "                                            col('Estimated Median Income') \\\n",
    "                                .cast('double'))\n",
    "    \n",
    "    max_income_zip_codes = crime_data_join_income.groupBy('Zip Code') \\\n",
    "                            .agg({'Estimated Median Income': 'max'}) \\\n",
    "                            .withColumnRenamed('max(Estimated Median Income)', 'MaxIncome') \\\n",
    "                            .orderBy(col('MaxIncome').desc()) \\\n",
    "                            .limit(3)\n",
    "    \n",
    "    min_income_zip_codes = crime_data_join_income.groupBy('Zip Code') \\\n",
    "                            .agg({'Estimated Median Income': 'min'}) \\\n",
    "                            .withColumnRenamed('min(Estimated Median Income)', 'MinIncome') \\\n",
    "                            .orderBy(col('MinIncome')) \\\n",
    "                            .limit(3)\n",
    "    \n",
    "    zip_codes = min_income_zip_codes.union(max_income_zip_codes)\n",
    "    \n",
    "    zip_codes_list = [row['Zip Code'] for row in zip_codes.collect()]\n",
    "    \n",
    "    result = crime_data_join_income \\\n",
    "                .filter(col('Zip Code').isin(zip_codes_list)) \\\n",
    "                .filter(year(col('Date Rptd')) == 2015) \\\n",
    "                .groupBy('Vict Descent') \\\n",
    "                .count() \\\n",
    "                .withColumnRenamed('count', '#') \\\n",
    "                .orderBy(col('#').desc())\n",
    "    \n",
    "    result.show()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    num_executors = spark.conf.get(\"spark.executor.instances\")\n",
    "    print(f'{num_executors} Executors')\n",
    "    \n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    \n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9f130-baf2-4694-bdd5-217787fbba3d",
   "metadata": {},
   "source": [
    "# Query 3 on 2 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8260c478-9b3f-4e9e-8d71-39cde541e5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1556|\n",
      "|           B|1092|\n",
      "|           W|1002|\n",
      "|           O| 484|\n",
      "|           A| 116|\n",
      "|           K|   7|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "2 Executors\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##613L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##613L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=984]\n",
      "      +- HashAggregate(keys=[Vict Descent#13], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(Vict Descent#13, 200), ENSURE_REQUIREMENTS, [plan_id=981]\n",
      "            +- HashAggregate(keys=[Vict Descent#13], functions=[partial_count(1)])\n",
      "               +- Project [Vict Descent#13]\n",
      "                  +- BroadcastHashJoin [Zip Code#318], [Zip Code#62], Inner, BuildRight, false\n",
      "                     :- Project [Vict Descent#13, cast(ZIPcode#58 as int) AS Zip Code#318]\n",
      "                     :  +- BroadcastHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#26)), knownfloatingpointnormalized(normalizenanandzero(LON#27))], [knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57))], Inner, BuildRight, false\n",
      "                     :     :- Project [Vict Descent#13, LAT#26, LON#27]\n",
      "                     :     :  +- Filter ((((((isnotnull(Vict Descent#13) AND isnotnull(Vict Sex#12)) AND NOT (Vict Descent#13 = X)) AND NOT (Vict Sex#12 = X)) AND (year(cast(gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) as date)) = 2015)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                     :     :     +- FileScan parquet [Date Rptd#1,Vict Sex#12,Vict Descent#13,LAT#26,LON#27] Batched: true, DataFilters: [isnotnull(Vict Descent#13), isnotnull(Vict Sex#12), NOT (Vict Descent#13 = X), NOT (Vict Sex#12 ..., Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [IsNotNull(`Vict Descent`), IsNotNull(`Vict Sex`), Not(EqualTo(`Vict Descent`,X)), Not(EqualTo(`V..., ReadSchema: struct<Date Rptd:string,Vict Sex:string,Vict Descent:string,LAT:double,LON:double>\n",
      "                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(knownfloatingpointnormalized(normalizenanandzero(input[0, double, false])), knownfloatingpointnormalized(normalizenanandzero(input[1, double, false]))),false), [plan_id=972]\n",
      "                     :        +- Filter (((cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274) AND isnotnull(LAT#56)) AND isnotnull(LON#57)) AND isnotnull(cast(ZIPcode#58 as int)))\n",
      "                     :           +- FileScan parquet [LAT#56,LON#57,ZIPcode#58] Batched: true, DataFilters: [cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274), isnotnull(LAT#56), isnotnull(L..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/revgecoding.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=976]\n",
      "                        +- Filter (Zip Code#62 IN (90021,90058,90013,90272,90077,90274) AND isnotnull(Zip Code#62))\n",
      "                           +- FileScan parquet [Zip Code#62] Batched: true, DataFilters: [Zip Code#62 IN (90021,90058,90013,90272,90077,90274), isnotnull(Zip Code#62)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet], PartitionFilters: [], PushedFilters: [In(`Zip Code`, [90013,90021,90058,90077,90272,90274]), IsNotNull(`Zip Code`)], ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "\n",
      "Method : CONTINUE | Time 17.03839087486267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.03839087486267"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7cc40a-866f-473d-a041-8777f3309771",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
