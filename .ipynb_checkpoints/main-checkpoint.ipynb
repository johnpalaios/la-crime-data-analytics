{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56c50ab-5769-4e7d-946c-ec870888801c",
   "metadata": {},
   "source": [
    "#### Useful Links \n",
    "- Spark History Server : http://83.212.73.248:18080/\n",
    "- Hadoop YARN (scheduler) : http://83.212.73.248:8088/cluster\n",
    "- HDFS : http://83.212.73.248:9870/dfshealth.html#tab-overview\n",
    "\n",
    "#### Useful Commands : \n",
    "- Connect to okeanos-master (from local) : `$ ssh user@snf-40202.ok-kno.grnetcloud.net `\n",
    "    - Password : 'Rand0m'\n",
    "- Connect to okeanos-worker (from okeanos-master) : `$ ssh okeanos-worker`\n",
    "- Open Jupyter Notebook : `$ jupyter notebook --ip 83.212.73.248 --port 8888`\n",
    "\n",
    "#### Thinks to do :\n",
    "- Make the data Csv to Parquet\n",
    "- Make those columns the type we want\n",
    "- Write the Queries (!)\n",
    "- Benchmark and optimize them etc.\n",
    "- Balance the data onto HDFS across the two datanodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667a612-a781-4680-bb05-0fdfe7aed047",
   "metadata": {},
   "source": [
    "### Full HDFS path is here : hdfs://okeanos-master:54310/csv_data/\n",
    "and contains :  \n",
    "     \n",
    "     1.  hdfs://okeanos-master:54310/csv_data/LAPD_Police_Stations.csv\n",
    "     2.  hdfs://okeanos-master:54310/csv_data/crime_data_2019.csv \n",
    "     3.  hdfs://okeanos-master:54310/csv_data/crime_data_2023.csv\n",
    "     4.  hdfs://okeanos-master:54310/csv_data/revgecoding.csv \n",
    "     5.  hdfs://okeanos-master:54310/csv_data/income/\n",
    "         1. LA_income_2015.csv\n",
    "         2. LA_income_2017.csv\n",
    "         3. LA_income_2019.csv\n",
    "         4. LA_income_2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a4e254-eda8-4782-a680-a9fc5c1d69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from operator import add\n",
    "import geopy.distance\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, rank\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbb858d-c65c-40f6-93c8-eae9504bbf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/28 21:46:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/28 21:46:06 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# initialize sparkSession, make the data from csv to parquet,\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"4 Executors\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efaaacbe-53a0-4f5d-bf6f-f6eb6c42e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data into memory, do the necessary joins etc. here\n",
    "crime_data = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/crime_data_*.parquet\")\n",
    "revge = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/revgecoding.parquet\")\n",
    "# only 2015 income data needed\n",
    "income = spark.read \\\n",
    "            .parquet(\"hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet\")\n",
    "lapd_stations = spark.read.parquet(\"hdfs://okeanos-master:54310/parquet/LAPD_Police_Stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b586a4fc-878b-4370-91e1-36faca721379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crime_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0098ff2-3857-4604-8d16-5c26d003759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ad4d5-4e37-41a5-8833-7b3969368d31",
   "metadata": {},
   "source": [
    "#### Change Column Types \n",
    "Στην εκφωνηση λέει : Διατηρώντας τα αρχικά ονόματα στηλών \n",
    "\n",
    "εννοωντας οτι δεν μπορουμε να κανουμε το 'Date Rptd' -> 'Date_Rptd' ?\n",
    "- Date Rptd: date\n",
    "- DATE OCC: date\n",
    "- Vict Age: integer\n",
    "- LAT: double\n",
    "- LON: double\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b85974b-2551-4dda-859a-e86e3011dcea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code for column type changing\n",
    "crime_data = crime_data.withColumn(\"Date Rptd\", to_timestamp(\"Date Rptd\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(\"DATE OCC\", 'MM/dd/yyyy hh:mm:ss a')) \\\n",
    "    .withColumn(\"Vict Age\", col(\"Vict Age\").cast(\"int\")) \\\n",
    "    .withColumn(\"LAT\", col(\"LAT\").cast(\"double\")) \\\n",
    "    .withColumn(\"LON\", col(\"LON\").cast(\"double\")) \\\n",
    "    .withColumn(\"Premis_Desc\", col(\"Premis Desc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7729edb-fff8-4a31-b7bd-4dd61a4ea834",
   "metadata": {},
   "source": [
    "# 1st Query :\r\n",
    "        find\r\n",
    "            for each year\r\n",
    "                the 3 months with the biggest crime count\r\n",
    "\r\n",
    "        year | month | crime_total (count)  + #order\r\n",
    "        dataframe.show()\r\n",
    "\r\n",
    "        SELECT  YEAR(date_rptd) as year,\r\n",
    "                MONTH(date_rptd) as month,\r\n",
    "                COUNT(*) as crime_total,\r\n",
    "                ROW_NUMBER() OVER (PARTITION BY year ORDER BY crime_total) as '#'\r\n",
    "        GROUP BY YEAR(date_rptd), MONTH(date_rptd)\r\n",
    "        SORT BY year ASC, crime_total DESp    GROUP BY police_station_name\r\n",
    "                ORDER BY #\r\n",
    "\r\n",
    "    2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db03eee5-f317-4051-b526-4092b7522187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for first query in SQL API\n",
    "def query_1_SQL_API():\n",
    "    start_time = time.time()\n",
    "    crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT \n",
    "                year(`Date Rptd`) AS year,\n",
    "                month(`Date Rptd`) AS month,\n",
    "                COUNT(*) AS crime_total,\n",
    "                ROW_NUMBER() OVER (PARTITION BY year(`Date Rptd`) ORDER BY COUNT(*) DESC) AS rank\n",
    "            FROM \n",
    "                crime_data\n",
    "            GROUP BY \n",
    "                year(`Date Rptd`), month(`Date Rptd`)\n",
    "        ) ranked_data\n",
    "        WHERE rank <= 3\n",
    "        ORDER BY year, rank\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    result_df = spark.sql(query)\n",
    "    \n",
    "    result_df.show()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    result_df.explain()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39898c69-f992-45c8-a534-8ee164828c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for first query in Dataframe API\n",
    "def query_1_Dataframe_API():\n",
    "    start_time = time.time()\n",
    "    crime_counts = crime_data.withColumn(\"year\", F.year(\"Date Rptd\")) \\\n",
    "                          .withColumn(\"month\", F.month(\"Date Rptd\")) \\\n",
    "                          .groupBy(\"year\", \"month\") \\\n",
    "                          .agg(F.count(\"*\").alias(\"crime_total\"))\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"year\").orderBy(F.desc(\"crime_total\"))\n",
    "    \n",
    "    ranked_crime = crime_counts.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    \n",
    "    result_df = ranked_crime.filter(\"rank <= 3\").orderBy(\"year\", \"rank\")\n",
    "    \n",
    "    result_df.show()\n",
    "    end_time = time.time()\n",
    "\n",
    "    result_df.explain()\n",
    "\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55eda9-3400-4cf9-a5a8-2247792fd456",
   "metadata": {},
   "source": [
    " # 2nd Query :\n",
    "\n",
    "\n",
    "            SELECT street,\n",
    "                   CASE\n",
    "                      WHEN HOUR('Date Rptd') BETWEEN 5 AND 11 THEN 'Morning'\n",
    "                      WHEN HOUR('Date Rptd') BETWEEN 12 AND 16 THEN 'Noon'\n",
    "                      WHEN HOUR('Date Rptd') BETWEEN 17 AND 20 THEN 'Afternoon'\n",
    "                      ELSE 'Night'\n",
    "                    END AS time_group,\n",
    "                    COUNT(*) as count\n",
    "            WHERE 'Prem Desc'='STREET'\n",
    "            GROUP BY time_group\n",
    "            ORDER BY count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab6c1df-26e6-4b93-b9ee-a2d5cff7a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 2nd query here for Dataframe/SQL API\n",
    "def query_2_Dataframe_API():\n",
    "    start_time = time.time()\n",
    "    filtered_df = crime_data.filter(crime_data['Premis_Desc'] == 'STREET')\n",
    "\n",
    "    time_group_df = filtered_df.withColumn(\"time_group\",\n",
    "                                       # TIME OCC is in 24 hour military time integer values\n",
    "                                      F.when((F.col('TIME OCC').between(500, 1159)), 'Morning')\n",
    "                                      .when((F.col('TIME OCC').between(1200, 1659)), 'Noon')\n",
    "                                      .when((F.col('TIME OCC').between(1700, 2059)), 'Afternoon')\n",
    "                                      .otherwise('Night'))\n",
    "\n",
    "    result_df = time_group_df.groupBy(\"time_group\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "    result_df = result_df.orderBy(col(\"count\").desc())\n",
    "    result_df.show()\n",
    "    end_time = time.time()\n",
    "    # call explain() method in order\n",
    "    # to see the query's physical plan\n",
    "    # and improve the RDD query\n",
    "    result_df.explain()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339e48bb-fb2f-4a6a-acca-c11c76786357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 2nd query here for RDD API\n",
    "def time_segs(row):\n",
    "    if 500 <= int(row['TIME OCC']) <= 1159:\n",
    "        return 'Morning'\n",
    "    elif 1200 <= int(row['TIME OCC']) <= 1659:\n",
    "        return 'Noon'\n",
    "    elif 1700 <= int(row['TIME OCC']) <= 2059:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f928f8cf-9407-4a3c-b0cc-836e78e49b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_2_rdd(): \n",
    "    start_time = time.time()\n",
    "    crime_data_rdd = crime_data.rdd.filter(lambda x: x['Premis_Desc'] == 'STREET') \\\n",
    "                                .map(lambda x: (time_segs(x),1)) \\\n",
    "                                .reduceByKey(lambda k1,k2: k1+k2) \\\n",
    "                                .sortBy(lambda x: x[1], ascending = False)\n",
    "    \n",
    "    result = crime_data_rdd.collect()\n",
    "    for time_of_day, count in result:\n",
    "        print(f\"{time_of_day}: {count}\")\n",
    "        \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c51ced-8b88-4c16-ad21-0b1479e9b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time_group(row):\n",
    "    time_occ = int(row)\n",
    "    if 500 <= time_occ <= 1159:\n",
    "        return 'Morning'\n",
    "    elif 1200 <= time_occ <= 1659:\n",
    "        return 'Noon'\n",
    "    elif 1700 <= time_occ <= 2059:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Night'\n",
    "        \n",
    "def query_2_rdd_new():\n",
    "    start_time = time.time()\n",
    "\n",
    "    crime_data_rdd = crime_data.rdd.filter(lambda x: x['Premis_Desc'] == 'STREET') \\\n",
    "                                .map(lambda x: (map_time_group(x['TIME OCC']), 1)) \\\n",
    "                                .reduceByKey(add).sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "    result = crime_data_rdd.collect()\n",
    "    for time_of_day, count in result:\n",
    "        print(f\"{time_of_day}: {count}\")\n",
    "        \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990b45bf-c7a1-4f7d-8a85-e8b7827380fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_2_rdd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc4ad40f-c002-49d6-bc68-230131410474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c962e-49f3-4409-aa0e-3b113c973570",
   "metadata": {},
   "source": [
    "# 3rd Query :\n",
    "\n",
    "        find the 3 zip codes with min and max household income\n",
    "                    |\n",
    "                    |\n",
    "                    v\n",
    "        // filter(remove) victimless crimes\n",
    "                    |\n",
    "                    |\n",
    "                    v\n",
    "        select vict_desc, COUNT(*) as count\n",
    "        where year=2015\n",
    "        group by vict_desc\n",
    "        order by count DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c006e0c2-4c2d-4541-ac68-10bad330ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 3rd query here\n",
    "def query_3(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    #crime_data_2015 = crime_data.filter(year(col('Date Rptd')) == 2015)\n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_revge = crime_data.join(broadcast(revge), ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        #crime_data_join_revge.explain()\n",
    "        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_revge = crime_data.hint(method).join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        #crime_data_join_revge.explain()\n",
    "        \n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_revge = crime_data.join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "            .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "            .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "            .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    #crime_data_join_revge = crime_data.join(revge, ['LAT', 'LON'], 'inner') \\\n",
    "    #    .withColumnRenamed('ZIPcode', 'Zip Code') \\\n",
    "    #    .withColumn(\"Zip Code\", col(\"Zip Code\").cast(\"int\")) \\\n",
    "    #    .filter((col('Vict Descent') != 'X') & (col('Vict Sex') != 'X'))\n",
    "    \n",
    "    crime_data_join_income = crime_data_join_revge.join(income, 'Zip Code', 'inner') \\\n",
    "                                .withColumn('Estimated Median Income', \n",
    "                                            regexp_replace(col('Estimated Median Income'), '[$,]', '')) \\\n",
    "                                .withColumn('Estimated Median Income', \n",
    "                                            col('Estimated Median Income') \\\n",
    "                                .cast('double'))\n",
    "    \n",
    "    max_income_zip_codes = crime_data_join_income.groupBy('Zip Code') \\\n",
    "                            .agg({'Estimated Median Income': 'max'}) \\\n",
    "                            .withColumnRenamed('max(Estimated Median Income)', 'MaxIncome') \\\n",
    "                            .orderBy(col('MaxIncome').desc()) \\\n",
    "                            .limit(3)\n",
    "    \n",
    "    min_income_zip_codes = crime_data_join_income.groupBy('Zip Code') \\\n",
    "                            .agg({'Estimated Median Income': 'min'}) \\\n",
    "                            .withColumnRenamed('min(Estimated Median Income)', 'MinIncome') \\\n",
    "                            .orderBy(col('MinIncome')) \\\n",
    "                            .limit(3)\n",
    "    \n",
    "    zip_codes = min_income_zip_codes.union(max_income_zip_codes)\n",
    "    \n",
    "    zip_codes_list = [row['Zip Code'] for row in zip_codes.collect()]\n",
    "    \n",
    "    result = crime_data_join_income \\\n",
    "                .filter(col('Zip Code').isin(zip_codes_list)) \\\n",
    "                .filter(year(col('Date Rptd')) == 2015) \\\n",
    "                .groupBy('Vict Descent') \\\n",
    "                .count() \\\n",
    "                .withColumnRenamed('count', '#') \\\n",
    "                .orderBy(col('#').desc())\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eefbc50-5aff-49e3-850e-f646dfde1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_3(\"SHUFFLE_HASH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06b66b81-6415-4e00-be9b-5fcec3537f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d07b41-53fe-4ad7-8b80-9efa43bbcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "#    query_3(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48047e1-8739-48ef-86b3-1cc547d2beb1",
   "metadata": {},
   "source": [
    "# 4th Query :\n",
    "    1 :\n",
    "        a)\n",
    "                make_extra_columns() : distance of police stations from crime\n",
    "                join crime_table with LA Police Stations on police_station\n",
    "                and add column of police station\n",
    "                for each row compute distance from two coordinates                         \n",
    "                put this computed distance in the column named 'distance'\n",
    "\n",
    "                SELECT year, SUM(distance)/# as average_distance\n",
    "                                , COUNT(*) as #\n",
    "                FROM ...\n",
    "                WHERE WEAPON < 200\n",
    "                GROUP BY year\n",
    "                ORDER BY #\n",
    "\n",
    "\n",
    "        b)\n",
    "                SELECT police_station_name as division,\n",
    "                       SUM(distance)/# as average_distance,\n",
    "                       COUNT(*) as #\n",
    "                FROM ...\n",
    "                WHERE weapon NOT NULL\n",
    "                GROUP BY police_station_name\n",
    "                ORDER BY #\n",
    "\n",
    "    2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d8c250-8820-4bb9-84de-b83bd512fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance on a sphere (as earth is not flat)\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371\n",
    "    dLat = math.radians(lat2 - lat1)\n",
    "    dLon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dLat / 2) * math.sin(dLat / 2) + \\\n",
    "        math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dLon / 2) * math.sin(dLon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "505a08e7-d25b-4e9c-af0c-c33b7c15f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for 4th query here\n",
    "# 1a)\n",
    "def query_4_1a(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    lapd_stations_new = lapd_stations.withColumnRenamed('PREC','AREA')\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(broadcast(lapd_stations_new), 'AREA', 'inner')\n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .hint(method).join(lapd_stations_new, 'AREA', 'inner')\n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(lapd_stations_new, 'AREA', 'inner')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('distance',\n",
    "                                    haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(col('Weapon Used Cd') < 200) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('year') \\\n",
    "        .agg((F.sum('distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')) \\\n",
    "       .orderBy(F.col('year'))\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1887d39-8279-4ee0-befe-2e1117b5d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b)\n",
    "def query_4_1b(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    lapd_stations_new = lapd_stations.withColumnRenamed('PREC','AREA')\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(broadcast(lapd_stations_new), 'AREA', 'inner')\n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .hint(method).join(lapd_stations_new, 'AREA', 'inner')\n",
    "    elif method == 'CONTINUE':\n",
    "        crime_data_join_stations = crime_data.withColumnRenamed('AREA ', 'AREA') \\\n",
    "                                    .join(lapd_stations_new, 'AREA', 'inner')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # distance of LAT and LON using Spark functions\n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('distance',\n",
    "                                    haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\"))) # Earths's radius\n",
    "    \n",
    "    crime_data_join_stations = crime_data_join_stations.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(F.col('Weapon Used Cd').isNotNull()) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('AREA NAME') \\\n",
    "        .agg(\n",
    "            (F.sum('distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')\n",
    "        ) \\\n",
    "        .orderBy(F.col('#').desc()) \\\n",
    "        .withColumnRenamed('AREA NAME', 'division')\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c5b1f43-dd42-4ab4-89fc-100b2de20849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DR_NO: integer (nullable = true)\n",
      " |-- Date Rptd: timestamp (nullable = true)\n",
      " |-- DATE OCC: timestamp (nullable = true)\n",
      " |-- TIME OCC: integer (nullable = true)\n",
      " |-- AREA : integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: integer (nullable = true)\n",
      " |-- Part 1-2: integer (nullable = true)\n",
      " |-- Crm Cd: integer (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: integer (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: integer (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: integer (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: integer (nullable = true)\n",
      " |-- Crm Cd 2: integer (nullable = true)\n",
      " |-- Crm Cd 3: integer (nullable = true)\n",
      " |-- Crm Cd 4: integer (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- Premis_Desc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "987774a5-2c90-4679-9a1d-3b3d7f93864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a)\n",
    "def query_4_2a(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        combined_data = crime_data.hint(method).crossJoin(broadcast(lapd_stations))        \n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        combined_data = crime_data.hint(method).crossJoin(lapd_stations)\n",
    "    elif method == 'CONTINUE':\n",
    "        combined_data = crime_data.crossJoin(lapd_stations)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # filter for DR_NO < 200  \n",
    "    # or Catalyst Optimizer does it by itself \n",
    "    \n",
    "    combined_data = combined_data.withColumn(\"closest_distance\", haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    windowSpec = Window.partitionBy(\"DR_NO\").orderBy(\"closest_distance\")\n",
    "    closest_stations = combined_data.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") == 1)\n",
    "    \n",
    "    final_data = closest_stations.select(col(\"DR_NO\"), col(\"DIVISION\").alias(\"closest_station\"), col(\"closest_distance\"))\n",
    "    \n",
    "    result = crime_data.join(final_data, \"DR_NO\")\n",
    "    crime_data_join_stations = result.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(col('Weapon Used Cd') < 200) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('year') \\\n",
    "        .agg((F.sum('closest_distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')) \\\n",
    "       .orderBy(F.col('year'))\n",
    "\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c27dfc9d-94e5-4d8c-874a-7a6872692fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b)\n",
    "def query_4_2b(method = 'CONTINUE'):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'BROADCAST':\n",
    "        combined_data = crime_data.hint(method).crossJoin(broadcast(lapd_stations))\n",
    "    elif method in ['MERGE', 'SHUFFLE_HASH', 'SHUFFLE_REPLICATE_NL']:\n",
    "        combined_data = crime_data.hint(method).crossJoin(lapd_stations)        \n",
    "    elif method == 'CONTINUE':\n",
    "        combined_data = crime_data.crossJoin(lapd_stations)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    combined_data = combined_data.withColumn(\"closest_distance\", haversine_udf(col(\"LON\"), col(\"LAT\"), col(\"X\"), col(\"Y\")))\n",
    "    \n",
    "    windowSpec = Window.partitionBy(\"DR_NO\").orderBy(\"closest_distance\")\n",
    "    closest_stations = combined_data.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") == 1)\n",
    "    \n",
    "    final_data = closest_stations.select(col(\"DR_NO\"), col(\"DIVISION\").alias(\"closest_station\"), col(\"closest_distance\"))\n",
    "    \n",
    "    result = crime_data.join(final_data, \"DR_NO\")\n",
    "    crime_data_join_stations = result.withColumn('Weapon Used Cd', col('Weapon Used Cd').cast('int')) \\\n",
    "                                    .filter(F.col('Weapon Used Cd').isNotNull()) \\\n",
    "                                    .withColumn('year', F.year('Date Rptd'))\n",
    "    \n",
    "    result = crime_data_join_stations.groupBy('AREA NAME') \\\n",
    "        .agg(\n",
    "            (F.sum('closest_distance') / F.count('*')).alias('average_distance'),\n",
    "            F.count('*').alias('#')\n",
    "        ) \\\n",
    "        .orderBy(F.col('#').desc()) \\\n",
    "        .withColumnRenamed('AREA NAME', 'division')\n",
    "    \n",
    "    result.show()\n",
    "    end_time = time.time()\n",
    "    result.explain()\n",
    "    print(f'Method : {method} | Time {end_time - start_time}')\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc31421-b725-4649-8b65-5f6308026e91",
   "metadata": {},
   "source": [
    "# Query 1 on 4 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1aedf41-fde4-4afb-b669-a85ff36c9bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/28 21:46:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[DR_NO: int, Date Rptd: timestamp, DATE OCC: timestamp, TIME OCC: int, AREA : int, AREA NAME: string, Rpt Dist No: int, Part 1-2: int, Crm Cd: int, Crm Cd Desc: string, Mocodes: string, Vict Age: int, Vict Sex: string, Vict Descent: string, Premis Cd: int, Premis Desc: string, Weapon Used Cd: int, Weapon Desc: string, Status: string, Status Desc: string, Crm Cd 1: int, Crm Cd 2: int, Crm Cd 3: int, Crm Cd 4: int, LOCATION: string, Cross Street: string, LAT: double, LON: double, Premis_Desc: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " crime_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2175a3f8-6285-4e40-8abe-f4d64c010ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----+\n",
      "|year|month|crime_total|rank|\n",
      "+----+-----+-----------+----+\n",
      "|2010|    3|      17595|   1|\n",
      "|2010|    7|      17520|   2|\n",
      "|2010|    5|      17338|   3|\n",
      "|2011|    8|      17139|   1|\n",
      "|2011|    5|      17050|   2|\n",
      "|2011|    3|      16951|   3|\n",
      "|2012|    8|      17696|   1|\n",
      "|2012|   10|      17477|   2|\n",
      "|2012|    5|      17391|   3|\n",
      "|2013|    8|      17329|   1|\n",
      "|2013|    7|      16714|   2|\n",
      "|2013|    5|      16671|   3|\n",
      "|2014|    7|      17456|   1|\n",
      "|2014|   10|      17300|   2|\n",
      "|2014|   12|      17076|   3|\n",
      "|2015|    8|      19134|   1|\n",
      "|2015|   10|      19065|   2|\n",
      "|2015|    7|      18755|   3|\n",
      "|2016|    8|      19834|   1|\n",
      "|2016|   10|      19678|   2|\n",
      "+----+-----+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [year#401 ASC NULLS FIRST, rank#501 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(year#401 ASC NULLS FIRST, rank#501 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=281]\n",
      "      +- Filter (rank#501 <= 3)\n",
      "         +- Window [row_number() windowspecdefinition(year#401, crime_total#496L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#501], [year#401], [crime_total#496L DESC NULLS LAST]\n",
      "            +- WindowGroupLimit [year#401], [crime_total#496L DESC NULLS LAST], row_number(), 3, Final\n",
      "               +- Sort [year#401 ASC NULLS FIRST, crime_total#496L DESC NULLS LAST], false, 0\n",
      "                  +- Exchange hashpartitioning(year#401, 200), ENSURE_REQUIREMENTS, [plan_id=275]\n",
      "                     +- WindowGroupLimit [year#401], [crime_total#496L DESC NULLS LAST], row_number(), 3, Partial\n",
      "                        +- Sort [year#401 ASC NULLS FIRST, crime_total#496L DESC NULLS LAST], false, 0\n",
      "                           +- HashAggregate(keys=[year#401, month#432], functions=[count(1)])\n",
      "                              +- Exchange hashpartitioning(year#401, month#432, 200), ENSURE_REQUIREMENTS, [plan_id=269]\n",
      "                                 +- HashAggregate(keys=[year#401, month#432], functions=[partial_count(1)])\n",
      "                                    +- Project [year(cast(Date Rptd#80 as date)) AS year#401, month(cast(Date Rptd#80 as date)) AS month#432]\n",
      "                                       +- InMemoryTableScan [Date Rptd#80]\n",
      "                                             +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                   +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                                                      +- *(1) ColumnarToRow\n",
      "                                                         +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.55947470664978"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1_Dataframe_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d0ad43-b4f9-4843-ae89-c2f49b003d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----+\n",
      "|year|month|crime_total|rank|\n",
      "+----+-----+-----------+----+\n",
      "|2010|    3|      17595|   1|\n",
      "|2010|    7|      17520|   2|\n",
      "|2010|    5|      17338|   3|\n",
      "|2011|    8|      17139|   1|\n",
      "|2011|    5|      17050|   2|\n",
      "|2011|    3|      16951|   3|\n",
      "|2012|    8|      17696|   1|\n",
      "|2012|   10|      17477|   2|\n",
      "|2012|    5|      17391|   3|\n",
      "|2013|    8|      17329|   1|\n",
      "|2013|    7|      16714|   2|\n",
      "|2013|    5|      16671|   3|\n",
      "|2014|    7|      17456|   1|\n",
      "|2014|   10|      17300|   2|\n",
      "|2014|   12|      17076|   3|\n",
      "|2015|    8|      19134|   1|\n",
      "|2015|   10|      19065|   2|\n",
      "|2015|    7|      18755|   3|\n",
      "|2016|    8|      19834|   1|\n",
      "|2016|   10|      19678|   2|\n",
      "+----+-----+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [year#1685 ASC NULLS FIRST, rank#1688 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(year#1685 ASC NULLS FIRST, rank#1688 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=587]\n",
      "      +- Project [year#1685, month#1686, crime_total#1687L, rank#1688]\n",
      "         +- Filter (rank#1688 <= 3)\n",
      "            +- Window [row_number() windowspecdefinition(_w0#1692, _w1#1693L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#1688], [_w0#1692], [_w1#1693L DESC NULLS LAST]\n",
      "               +- WindowGroupLimit [_w0#1692], [_w1#1693L DESC NULLS LAST], row_number(), 3, Final\n",
      "                  +- Sort [_w0#1692 ASC NULLS FIRST, _w1#1693L DESC NULLS LAST], false, 0\n",
      "                     +- Exchange hashpartitioning(_w0#1692, 200), ENSURE_REQUIREMENTS, [plan_id=580]\n",
      "                        +- WindowGroupLimit [_w0#1692], [_w1#1693L DESC NULLS LAST], row_number(), 3, Partial\n",
      "                           +- Sort [_w0#1692 ASC NULLS FIRST, _w1#1693L DESC NULLS LAST], false, 0\n",
      "                              +- HashAggregate(keys=[_groupingexpression#2734, _groupingexpression#2735], functions=[count(1)])\n",
      "                                 +- Exchange hashpartitioning(_groupingexpression#2734, _groupingexpression#2735, 200), ENSURE_REQUIREMENTS, [plan_id=574]\n",
      "                                    +- HashAggregate(keys=[_groupingexpression#2734, _groupingexpression#2735], functions=[partial_count(1)])\n",
      "                                       +- Project [year(cast(Date Rptd#80 as date)) AS _groupingexpression#2734, month(cast(Date Rptd#80 as date)) AS _groupingexpression#2735]\n",
      "                                          +- InMemoryTableScan [Date Rptd#80]\n",
      "                                                +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                                      +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                                                         +- *(1) ColumnarToRow\n",
      "                                                            +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9540205001831055"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1_SQL_API()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e5649-89cf-4d22-bcca-f0c76f039ddf",
   "metadata": {},
   "source": [
    "# Query 2 on 4 Executors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98250c68-967e-46d6-a097-cf0e07c9b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|time_group| count|\n",
      "+----------+------+\n",
      "|     Night|239524|\n",
      "| Afternoon|188668|\n",
      "|      Noon|148961|\n",
      "|   Morning|124397|\n",
      "+----------+------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#2943L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#2943L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=705]\n",
      "      +- HashAggregate(keys=[time_group#2881], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(time_group#2881, 200), ENSURE_REQUIREMENTS, [plan_id=702]\n",
      "            +- HashAggregate(keys=[time_group#2881], functions=[partial_count(1)])\n",
      "               +- Project [CASE WHEN ((TIME OCC#3 >= 500) AND (TIME OCC#3 <= 1159)) THEN Morning WHEN ((TIME OCC#3 >= 1200) AND (TIME OCC#3 <= 1659)) THEN Noon WHEN ((TIME OCC#3 >= 1700) AND (TIME OCC#3 <= 2059)) THEN Afternoon ELSE Night END AS time_group#2881]\n",
      "                  +- Filter (isnotnull(Premis_Desc#226) AND (Premis_Desc#226 = STREET))\n",
      "                     +- InMemoryTableScan [Premis_Desc#226, TIME OCC#3], [isnotnull(Premis_Desc#226), (Premis_Desc#226 = STREET)]\n",
      "                           +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                 +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                                    +- *(1) ColumnarToRow\n",
      "                                       +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5807712078094482"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2_Dataframe_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05b4d4b6-a5ca-4d86-93c0-72355d45a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night: 239524\n",
      "Afternoon: 188668\n",
      "Noon: 148961\n",
      "Morning: 124397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.750932216644287"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2_rdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a199f4e-af1a-4044-aba1-9de3e6dcd2f1",
   "metadata": {},
   "source": [
    "# Query 3 on 4 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c4e344e-6eab-480b-b6f8-920f5a78291f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1579|\n",
      "|           B|1114|\n",
      "|           W|1020|\n",
      "|           O| 494|\n",
      "|           A| 117|\n",
      "|           K|   8|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##6504L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##6504L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1820]\n",
      "      +- HashAggregate(keys=[Vict Descent#13], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(Vict Descent#13, 200), ENSURE_REQUIREMENTS, [plan_id=1817]\n",
      "            +- HashAggregate(keys=[Vict Descent#13], functions=[partial_count(1)])\n",
      "               +- Project [Vict Descent#13]\n",
      "                  +- BroadcastHashJoin [Zip Code#4759], [Zip Code#62], Inner, BuildRight, false\n",
      "                     :- Project [Vict Descent#13, cast(ZIPcode#58 as int) AS Zip Code#4759]\n",
      "                     :  +- BroadcastHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#168)), knownfloatingpointnormalized(normalizenanandzero(LON#197))], [knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57))], Inner, BuildRight, false\n",
      "                     :     :- Project [Vict Descent#13, LAT#168, LON#197]\n",
      "                     :     :  +- Filter (((((((isnotnull(Vict Descent#13) AND isnotnull(Vict Sex#12)) AND isnotnull(Date Rptd#80)) AND NOT (Vict Descent#13 = X)) AND NOT (Vict Sex#12 = X)) AND (year(cast(Date Rptd#80 as date)) = 2015)) AND isnotnull(LAT#168)) AND isnotnull(LON#197))\n",
      "                     :     :     +- InMemoryTableScan [Date Rptd#80, LAT#168, LON#197, Vict Descent#13, Vict Sex#12], [isnotnull(Vict Descent#13), isnotnull(Vict Sex#12), isnotnull(Date Rptd#80), NOT (Vict Descent#13 = X), NOT (Vict Sex#12 = X), (year(cast(Date Rptd#80 as date)) = 2015), isnotnull(LAT#168), isnotnull(LON#197)]\n",
      "                     :     :           +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     :     :                 +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                     :     :                    +- *(1) ColumnarToRow\n",
      "                     :     :                       +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(knownfloatingpointnormalized(normalizenanandzero(input[0, double, false])), knownfloatingpointnormalized(normalizenanandzero(input[1, double, false]))),false), [plan_id=1808]\n",
      "                     :        +- Filter (((cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274) AND isnotnull(LAT#56)) AND isnotnull(LON#57)) AND isnotnull(cast(ZIPcode#58 as int)))\n",
      "                     :           +- FileScan parquet [LAT#56,LON#57,ZIPcode#58] Batched: true, DataFilters: [cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274), isnotnull(LAT#56), isnotnull(L..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/revgecoding.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1812]\n",
      "                        +- Filter (Zip Code#62 IN (90021,90058,90013,90272,90077,90274) AND isnotnull(Zip Code#62))\n",
      "                           +- FileScan parquet [Zip Code#62] Batched: true, DataFilters: [Zip Code#62 IN (90021,90058,90013,90272,90077,90274), isnotnull(Zip Code#62)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet], PartitionFilters: [], PushedFilters: [In(`Zip Code`, [90013,90021,90058,90077,90272,90274]), IsNotNull(`Zip Code`)], ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "\n",
      "Method : CONTINUE | Time 7.472313642501831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.472313642501831"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b6323b9-6b57-475f-8595-aaaeb1223108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1579|\n",
      "|           B|1114|\n",
      "|           W|1020|\n",
      "|           O| 494|\n",
      "|           A| 117|\n",
      "|           K|   8|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##9483L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##9483L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2856]\n",
      "      +- HashAggregate(keys=[Vict Descent#13], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(Vict Descent#13, 200), ENSURE_REQUIREMENTS, [plan_id=2853]\n",
      "            +- HashAggregate(keys=[Vict Descent#13], functions=[partial_count(1)])\n",
      "               +- Project [Vict Descent#13]\n",
      "                  +- BroadcastHashJoin [Zip Code#7739], [Zip Code#62], Inner, BuildRight, false\n",
      "                     :- Project [Vict Descent#13, cast(ZIPcode#58 as int) AS Zip Code#7739]\n",
      "                     :  +- BroadcastHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#168)), knownfloatingpointnormalized(normalizenanandzero(LON#197))], [knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57))], Inner, BuildRight, false\n",
      "                     :     :- Project [Vict Descent#13, LAT#168, LON#197]\n",
      "                     :     :  +- Filter (((((((isnotnull(Vict Descent#13) AND isnotnull(Vict Sex#12)) AND isnotnull(Date Rptd#80)) AND NOT (Vict Descent#13 = X)) AND NOT (Vict Sex#12 = X)) AND (year(cast(Date Rptd#80 as date)) = 2015)) AND isnotnull(LAT#168)) AND isnotnull(LON#197))\n",
      "                     :     :     +- InMemoryTableScan [Date Rptd#80, LAT#168, LON#197, Vict Descent#13, Vict Sex#12], [isnotnull(Vict Descent#13), isnotnull(Vict Sex#12), isnotnull(Date Rptd#80), NOT (Vict Descent#13 = X), NOT (Vict Sex#12 = X), (year(cast(Date Rptd#80 as date)) = 2015), isnotnull(LAT#168), isnotnull(LON#197)]\n",
      "                     :     :           +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     :     :                 +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                     :     :                    +- *(1) ColumnarToRow\n",
      "                     :     :                       +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(knownfloatingpointnormalized(normalizenanandzero(input[0, double, false])), knownfloatingpointnormalized(normalizenanandzero(input[1, double, false]))),false), [plan_id=2844]\n",
      "                     :        +- Filter (((cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274) AND isnotnull(LAT#56)) AND isnotnull(LON#57)) AND isnotnull(cast(ZIPcode#58 as int)))\n",
      "                     :           +- FileScan parquet [LAT#56,LON#57,ZIPcode#58] Batched: true, DataFilters: [cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274), isnotnull(LAT#56), isnotnull(L..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/revgecoding.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2848]\n",
      "                        +- Filter (Zip Code#62 IN (90021,90058,90013,90272,90077,90274) AND isnotnull(Zip Code#62))\n",
      "                           +- FileScan parquet [Zip Code#62] Batched: true, DataFilters: [Zip Code#62 IN (90021,90058,90013,90272,90077,90274), isnotnull(Zip Code#62)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet], PartitionFilters: [], PushedFilters: [In(`Zip Code`, [90013,90021,90058,90077,90272,90274]), IsNotNull(`Zip Code`)], ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "\n",
      "Method : BROADCAST | Time 4.549721717834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1579|\n",
      "|           B|1114|\n",
      "|           W|1020|\n",
      "|           O| 494|\n",
      "|           A| 117|\n",
      "|           K|   8|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##12754L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##12754L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=4404]\n",
      "      +- HashAggregate(keys=[Vict Descent#13], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(Vict Descent#13, 200), ENSURE_REQUIREMENTS, [plan_id=4401]\n",
      "            +- HashAggregate(keys=[Vict Descent#13], functions=[partial_count(1)])\n",
      "               +- Project [Vict Descent#13]\n",
      "                  +- BroadcastHashJoin [Zip Code#10718], [Zip Code#62], Inner, BuildRight, false\n",
      "                     :- Project [Vict Descent#13, cast(ZIPcode#58 as int) AS Zip Code#10718]\n",
      "                     :  +- SortMergeJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#168)), knownfloatingpointnormalized(normalizenanandzero(LON#197))], [knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57))], Inner\n",
      "                     :     :- Sort [knownfloatingpointnormalized(normalizenanandzero(LAT#168)) ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(LON#197)) ASC NULLS FIRST], false, 0\n",
      "                     :     :  +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#168)), knownfloatingpointnormalized(normalizenanandzero(LON#197)), 200), ENSURE_REQUIREMENTS, [plan_id=4389]\n",
      "                     :     :     +- Project [Vict Descent#13, LAT#168, LON#197]\n",
      "                     :     :        +- Filter (((((((isnotnull(Vict Descent#13) AND isnotnull(Vict Sex#12)) AND isnotnull(Date Rptd#80)) AND NOT (Vict Descent#13 = X)) AND NOT (Vict Sex#12 = X)) AND (year(cast(Date Rptd#80 as date)) = 2015)) AND isnotnull(LAT#168)) AND isnotnull(LON#197))\n",
      "                     :     :           +- InMemoryTableScan [Date Rptd#80, LAT#168, LON#197, Vict Descent#13, Vict Sex#12], [isnotnull(Vict Descent#13), isnotnull(Vict Sex#12), isnotnull(Date Rptd#80), NOT (Vict Descent#13 = X), NOT (Vict Sex#12 = X), (year(cast(Date Rptd#80 as date)) = 2015), isnotnull(LAT#168), isnotnull(LON#197)]\n",
      "                     :     :                 +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     :     :                       +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                     :     :                          +- *(1) ColumnarToRow\n",
      "                     :     :                             +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "                     :     +- Sort [knownfloatingpointnormalized(normalizenanandzero(LAT#56)) ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(LON#57)) ASC NULLS FIRST], false, 0\n",
      "                     :        +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57)), 200), ENSURE_REQUIREMENTS, [plan_id=4390]\n",
      "                     :           +- Filter (((cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274) AND isnotnull(LAT#56)) AND isnotnull(LON#57)) AND isnotnull(cast(ZIPcode#58 as int)))\n",
      "                     :              +- FileScan parquet [LAT#56,LON#57,ZIPcode#58] Batched: true, DataFilters: [cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274), isnotnull(LAT#56), isnotnull(L..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/revgecoding.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4396]\n",
      "                        +- Filter (Zip Code#62 IN (90021,90058,90013,90272,90077,90274) AND isnotnull(Zip Code#62))\n",
      "                           +- FileScan parquet [Zip Code#62] Batched: true, DataFilters: [Zip Code#62 IN (90021,90058,90013,90272,90077,90274), isnotnull(Zip Code#62)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet], PartitionFilters: [], PushedFilters: [In(`Zip Code`, [90013,90021,90058,90077,90272,90274]), IsNotNull(`Zip Code`)], ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "\n",
      "Method : MERGE | Time 8.924407005310059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|Vict Descent|   #|\n",
      "+------------+----+\n",
      "|           H|1579|\n",
      "|           B|1114|\n",
      "|           W|1020|\n",
      "|           O| 494|\n",
      "|           A| 117|\n",
      "|           K|   8|\n",
      "|           J|   3|\n",
      "|           I|   3|\n",
      "|           C|   2|\n",
      "|           F|   1|\n",
      "+------------+----+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##16315L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##16315L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=5690]\n",
      "      +- HashAggregate(keys=[Vict Descent#13], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(Vict Descent#13, 200), ENSURE_REQUIREMENTS, [plan_id=5687]\n",
      "            +- HashAggregate(keys=[Vict Descent#13], functions=[partial_count(1)])\n",
      "               +- Project [Vict Descent#13]\n",
      "                  +- BroadcastHashJoin [Zip Code#14279], [Zip Code#62], Inner, BuildRight, false\n",
      "                     :- Project [Vict Descent#13, cast(ZIPcode#58 as int) AS Zip Code#14279]\n",
      "                     :  +- ShuffledHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#168)), knownfloatingpointnormalized(normalizenanandzero(LON#197))], [knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57))], Inner, BuildLeft\n",
      "                     :     :- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#168)), knownfloatingpointnormalized(normalizenanandzero(LON#197)), 200), ENSURE_REQUIREMENTS, [plan_id=5677]\n",
      "                     :     :  +- Project [Vict Descent#13, LAT#168, LON#197]\n",
      "                     :     :     +- Filter (((((((isnotnull(Vict Descent#13) AND isnotnull(Vict Sex#12)) AND isnotnull(Date Rptd#80)) AND NOT (Vict Descent#13 = X)) AND NOT (Vict Sex#12 = X)) AND (year(cast(Date Rptd#80 as date)) = 2015)) AND isnotnull(LAT#168)) AND isnotnull(LON#197))\n",
      "                     :     :        +- InMemoryTableScan [Date Rptd#80, LAT#168, LON#197, Vict Descent#13, Vict Sex#12], [isnotnull(Vict Descent#13), isnotnull(Vict Sex#12), isnotnull(Date Rptd#80), NOT (Vict Descent#13 = X), NOT (Vict Sex#12 = X), (year(cast(Date Rptd#80 as date)) = 2015), isnotnull(LAT#168), isnotnull(LON#197)]\n",
      "                     :     :              +- InMemoryRelation [DR_NO#0, Date Rptd#80, DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#139, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     :     :                    +- *(1) Project [DR_NO#0, gettimestamp(Date Rptd#1, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS Date Rptd#80, gettimestamp(DATE OCC#2, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) AS DATE OCC#110, TIME OCC#3, AREA #4, AREA NAME#5, Rpt Dist No#6, Part 1-2#7, Crm Cd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, Vict Sex#12, Vict Descent#13, Premis Cd#14, Premis Desc#15, Weapon Used Cd#16, Weapon Desc#17, Status#18, Status Desc#19, Crm Cd 1#20, Crm Cd 2#21, Crm Cd 3#22, Crm Cd 4#23, ... 5 more fields]\n",
      "                     :     :                       +- *(1) ColumnarToRow\n",
      "                     :     :                          +- FileScan parquet [DR_NO#0,Date Rptd#1,DATE OCC#2,TIME OCC#3,AREA #4,AREA NAME#5,Rpt Dist No#6,Part 1-2#7,Crm Cd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,Vict Sex#12,Vict Descent#13,Premis Cd#14,Premis Desc#15,Weapon Used Cd#16,Weapon Desc#17,Status#18,Status Desc#19,Crm Cd 1#20,Crm Cd 2#21,Crm Cd 3#22,Crm Cd 4#23,... 4 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(2 paths)[hdfs://okeanos-master:54310/parquet/crime_data_2019.parquet, hdfs://ok..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DR_NO:int,Date Rptd:string,DATE OCC:string,TIME OCC:int,AREA :int,AREA NAME:string,Rpt Dis...\n",
      "                     :     +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#56)), knownfloatingpointnormalized(normalizenanandzero(LON#57)), 200), ENSURE_REQUIREMENTS, [plan_id=5678]\n",
      "                     :        +- Filter (((cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274) AND isnotnull(LAT#56)) AND isnotnull(LON#57)) AND isnotnull(cast(ZIPcode#58 as int)))\n",
      "                     :           +- FileScan parquet [LAT#56,LON#57,ZIPcode#58] Batched: true, DataFilters: [cast(ZIPcode#58 as int) IN (90021,90058,90013,90272,90077,90274), isnotnull(LAT#56), isnotnull(L..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/revgecoding.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=5682]\n",
      "                        +- Filter (Zip Code#62 IN (90021,90058,90013,90272,90077,90274) AND isnotnull(Zip Code#62))\n",
      "                           +- FileScan parquet [Zip Code#62] Batched: true, DataFilters: [Zip Code#62 IN (90021,90058,90013,90272,90077,90274), isnotnull(Zip Code#62)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/parquet/income/LA_income_2015.parquet], PartitionFilters: [], PushedFilters: [In(`Zip Code`, [90013,90021,90058,90077,90272,90274]), IsNotNull(`Zip Code`)], ReadSchema: struct<Zip Code:int>\n",
      "\n",
      "\n",
      "Method : SHUFFLE_HASH | Time 9.50604248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.>                (0 + 0) / 6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x7fbe8e3756c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1359, in <lambda>\n",
      "    lambda wr, cc=self._gateway_client, id=self._target_id:\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBROADCAST\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMERGE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSHUFFLE_HASH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSHUFFLE_REPLICATE_NL\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mquery_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 54\u001b[0m, in \u001b[0;36mquery_3\u001b[0;34m(method)\u001b[0m\n\u001b[1;32m     46\u001b[0m min_income_zip_codes \u001b[38;5;241m=\u001b[39m crime_data_join_income\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZip Code\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     47\u001b[0m                         \u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstimated Median Income\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m}) \\\n\u001b[1;32m     48\u001b[0m                         \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin(Estimated Median Income)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinIncome\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     49\u001b[0m                         \u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinIncome\u001b[39m\u001b[38;5;124m'\u001b[39m)) \\\n\u001b[1;32m     50\u001b[0m                         \u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     52\u001b[0m zip_codes \u001b[38;5;241m=\u001b[39m min_income_zip_codes\u001b[38;5;241m.\u001b[39munion(max_income_zip_codes)\n\u001b[0;32m---> 54\u001b[0m zip_codes_list \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZip Code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mzip_codes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     56\u001b[0m result \u001b[38;5;241m=\u001b[39m crime_data_join_income \\\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZip Code\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misin(zip_codes_list)) \\\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;241m.\u001b[39mfilter(year(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate Rptd\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2015\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc())\n\u001b[1;32m     64\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/spark/python/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_3(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfaa40-3215-4fb1-9c20-d6e1831c26ba",
   "metadata": {},
   "source": [
    "# Query 4 on 4 Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f3227-7e31-41ce-9fe2-3ad13c94bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>  (0 + 4) / 6][Stage 103:>  (0 + 0) / 6][Stage 104:>  (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "query_4_1a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c2627-39c0-4a25-aedc-b45189117425",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4_1b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fccae6-016c-4a45-8a07-888ea44ae9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4_2a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029fb8d-319d-4e31-a8e6-69b97c06722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4_2b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9b5b2-94ae-4cb5-80d2-cb8f8775152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_1a(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffad6f-3f43-4660-98ff-44ff85fd42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_1b(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15089bd1-65f3-47b0-a2ae-1b1e5030aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','SHUFFLE_REPLICATE_NL']:\n",
    "    query_4_2a(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d14338-4bda-41cb-b43d-5731af568ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['BROADCAST','MERGE', 'SHUFFLE_HASH']:\n",
    "    query_4_2b(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08674079-e23f-46b1-b5b9-30c9c3f868a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                (0 + 4) / 6][Stage 103:>                (0 + 0) / 6]\r"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f8e2a-a778-4a09-87cf-9372e0c1de12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
